{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9a645c",
   "metadata": {},
   "source": [
    "# NER Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc921994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    ### This code was partially inspired by code included in the HLT course, obtained from https://github.com/cltl/ma-hlt-labs/, accessed in May 2020.\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #check for cases where empty lines mark sentence boundaries (which some conll files do).\n",
    "        if len(row) > 3:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a5922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_features(inputfile,word_embedding_model):\n",
    "    '''\n",
    "    This function extracts features from embeddings\n",
    "    \n",
    "    :param inputfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    '''\n",
    "    ### This code was partially inspired by code included in the HLT course, obtained from https://github.com/cltl/ma-hlt-labs/, accessed in May 2020.\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(inputfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        #check for cases where empty lines mark sentence boundaries (which some conll files do).\n",
    "        if len(row) > 3:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indexes = {'token': 0, 'pos': 1, 'tag': 2, 'previous': 4, 'latter': 5, 'capitals': 6,'stemm':7,'lemma':8}\n",
    "\n",
    "def extract_features_and_selected_labels(trainingfile, selected_features):\n",
    "    '''\n",
    "    Extract features and gold labels from a preprocessed file with the training data and return them as lists\n",
    "    \n",
    "    :param trainingfile: path to training file\n",
    "    :param selected_features: list of features that will be used to train the model\n",
    "    \n",
    "    :type trainingfile: string\n",
    "    :type selected_features: list\n",
    "    \n",
    "    :return features: features as a list of dictionaries\n",
    "    :return gold_labels: list of gold labels\n",
    "    '''\n",
    "    features = []\n",
    "    gold_labels = []\n",
    "    \n",
    "    conllinput = open(trainingfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        feature_value = {}\n",
    "        # Only extract the selected features\n",
    "        for feature_name in selected_features:\n",
    "            row_index = feature_indexes.get(feature_name)\n",
    "            feature_value[feature_name] = row[row_index]\n",
    "        features.append(feature_value)\n",
    "        \n",
    "        # Gold is in the third column\n",
    "        gold_labels.append(row[3])\n",
    "                \n",
    "    return features, gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43d5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(trainingfile):\n",
    "    \n",
    "    data = []\n",
    "    targets = []\n",
    "    # TIP: recall that you can find information on how to integrate features here:\n",
    "    # https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    with open(trainingfile, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                token = components[0]\n",
    "                feature_dict = {'token':token}\n",
    "                data.append(feature_dict)\n",
    "                #gold is in the last column\n",
    "                targets.append(components[-1])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ac0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_index = {'token': 0, 'pos': 1, 'tag': 2, 'previous': 4, 'latter': 5, 'capitals': 6,'stemm':7,'lemma':8}\n",
    "\n",
    "def extract_features(testfile, selected_features):\n",
    "    '''Extract features from a preprocessed file with the test data and return them as a list\n",
    "    \n",
    "    :param trainingfile: path to test file\n",
    "    :param selected_features: list of features that were selected to train the model\n",
    "    \n",
    "    :type testfile: string\n",
    "    :type selected_features: list\n",
    "    \n",
    "    :return features: features as a list of dictionaries'''\n",
    "\n",
    "    features = []\n",
    "    gold_labels = []\n",
    "    \n",
    "    conllinput = open(testfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        feature_value = {}\n",
    "        for feature_name in selected_features:\n",
    "            row_index = feature_to_index.get(feature_name)\n",
    "            feature_value[feature_name] = row[row_index]\n",
    "        features.append(feature_value)\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efaa1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(train_features, train_targets, modelname):\n",
    "    \n",
    "    '''Create a classifier and train it with vectorized features and corresponding gold labels\n",
    "    \n",
    "    input train_features: features to be transformed into vectors\n",
    "    input train_labels: gold labels corresponding to features\n",
    "    input modelname: name of the model that will be trained\n",
    "    \n",
    "    output model: trained classifier\n",
    "    output vec: DictVectorizer'''\n",
    "   \n",
    "    if modelname ==  'logreg':\n",
    "        # TIP: you may need to solve this: https://stackoverflow.com/questions/61814494/what-is-this-warning-convergencewarning-lbfgs-failed-to-converge-status-1\n",
    "        model = LogisticRegression(max_iter=10000)\n",
    "    if modelname == 'NB':\n",
    "        model = MultinomialNB()\n",
    "    if modelname == 'SVM':\n",
    "        model = LinearSVC(max_iter=10000)\n",
    "        \n",
    "    vec = DictVectorizer()\n",
    "    \n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model.fit(features_vectorized, train_targets)\n",
    "    \n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f894d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_embeddings(train_features, train_labels):\n",
    "    '''\n",
    "    Create an SVM classifier and train it with vectorized features and corresponding gold labels\n",
    "    \n",
    "    input train_features: features to be transformed into vectors\n",
    "    input train_labels: gold labels corresponding to features\n",
    "    \n",
    "    output model: trained classifier\n",
    "    '''\n",
    "\n",
    "    model = LinearSVC(max_iter=10000)\n",
    "    model.fit(train_features, train_labels)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93cea902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, inputdata, outputfile,selected_features):\n",
    "  \n",
    "    # Extracting features from input data\n",
    "    features = extract_features(inputdata,selected_features)\n",
    "    features = vec.transform(features)\n",
    "    \n",
    "    # Making prediction\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    # Writing the results\n",
    "    outfile = open(outputfile, 'w')\n",
    "    counter = 0\n",
    "    for line in open(inputdata, 'r'):\n",
    "        if len(line.rstrip('\\n').split()) > 0:\n",
    "            outfile.write(line.rstrip('\\n') + '\\t' + predictions[counter] + '\\n')\n",
    "            counter += 1\n",
    "    outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "251898a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data_embeddings(model, inputdata, outputfile, word_embedding_model):\n",
    "    '''\n",
    "    This function creates a classifier for making predictions embedded data\n",
    "    \n",
    "    input model: classifier that will make predictions\n",
    "    input inputdata: path to input data\n",
    "    input outputfile: path to output file, where the predictions for each feature will be written\n",
    "    input word_embedding_model : embedding model\n",
    "    '''\n",
    "    # extracting features\n",
    "    features = extract_embeddings_features(inputdata,word_embedding_model)\n",
    "    \n",
    "    # making predictions with extracted features\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    # Write results to an outputfile\n",
    "    outfile = open(outputfile, 'w')\n",
    "    counter = 0\n",
    "    for line in open(inputdata, 'r'):\n",
    "        if len(line.rstrip('\\n').split()) > 0:\n",
    "            outfile.write(line.rstrip('\\n') + '\\t' + predictions[counter] + '\\n')\n",
    "            counter += 1\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "201eeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(system_type, argv=None):\n",
    "    \n",
    "    #a very basic way for picking up commandline arguments\n",
    "    if argv is None:\n",
    "        argv = sys.argv    \n",
    "    \n",
    "    trainingfile = \"/Users/orbaytopal/Desktop/VUAI/Master/Machine learning NLP/ma-ml4nlp-labs-main/data/conll2003.train.conll\"\n",
    "    inputfile = \"/Users/orbaytopal/Desktop/VUAI/Master/Machine learning NLP/ma-ml4nlp-labs-main/data/conll2003.dev.conll\"\n",
    "    outputfile = \"output.conll2003_features\"\n",
    "    language_model = \"/Users/orbaytopal/Desktop/VUAI/Master/Machine learning NLP/ma-ml4nlp-labs-main/data/GoogleNews-vectors-negative300.bin\"\n",
    "    \n",
    "    \n",
    "    if system_type == \"with_features\":\n",
    "    # selecting features to train the model\n",
    "        selected_features = [\"token\",\"pos\",\"tag\",\"previous\",\"latter\",\"capitals\",\"stemm\",\"lemma\"]\n",
    "\n",
    "        # getting the selected training features and gold labels\n",
    "        training_features, gold_labels = extract_features_and_selected_labels(trainingfile,selected_features)\n",
    "\n",
    "        # Training three different models with the features, \n",
    "        # Classifying the data and writing the result to new conll files\n",
    "        for modelname in ['logreg', 'NB', 'SVM']:\n",
    "\n",
    "            ml_model, vec = create_classifier(training_features, gold_labels, modelname)\n",
    "            classify_data(ml_model, vec, inputfile, outputfile.replace('.conll','.' + modelname + '.conll'),selected_features)\n",
    "\n",
    "            dataframe = pd.read_table(outputfile.replace('.conll','.' + modelname + '.conll'))\n",
    "            dataframe = dataframe.set_axis([*dataframe.columns[:-1], 'NER2'], axis=1, inplace=False)\n",
    "            dataframe.to_csv(outputfile.replace('.conll','.' + modelname + '.conll'), sep='\\t')\n",
    "        \n",
    "    elif system_type == \"word_embeddings\":\n",
    "\n",
    "    # creating a language model\n",
    "        language_model = gensim.models.KeyedVectors.load_word2vec_format(language_model, binary=True)\n",
    "\n",
    "        # extracting the features and gold label\n",
    "        training_features, gold_labels = extract_embeddings_as_features_and_gold(trainingfile, language_model)\n",
    "\n",
    "        # creating the classification model\n",
    "        ml_model = create_classifier_embeddings(training_features[:1], gold_labels[:1])\n",
    "        classify_data_embeddings(ml_model, inputfile, outputfile.replace('.conll','.embedded.conll'), language_model)\n",
    "\n",
    "        data_frame = pd.read_table(outputfile.replace('.conll','.embedded.conll'))\n",
    "        data_frame = data_frame.set_axis([*data_frame.columns[:-1], 'NER2'], axis=1, inplace=False)\n",
    "        data_frame.to_csv(outputfile.replace('.conll','.embedded2.conll'), sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84607a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main(system_type=\"with_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afd009a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6cab6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingfile = \"datas/conll2003.train.conll\"\n",
    "inputfile = \"datas/conll2003.dev.conll\"\n",
    "outputfile = \"output.conll2003_features\"\n",
    "language_model = \"models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "language_model = gensim.models.KeyedVectors.load_word2vec_format(language_model, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77287bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train\n",
    "training_features, gold_labels = extract_embeddings_as_features_and_gold(trainingfile, language_model)\n",
    "# X_test, y_test\n",
    "test_features, tests_gold_labels = extract_embeddings_as_features_and_gold(inputfile, language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8b8adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(max_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d3ad74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(max_iter=3000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=3000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(max_iter=3000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_features[:100000], gold_labels[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38bdb81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1430    22   177    32    10     3    19    36   108]\n",
      " [   41   644    36    27     1    15    11    11   136]\n",
      " [   99    51   886    32     6     7    59    52   149]\n",
      " [   26     7    17  1303     3     3     2   365   116]\n",
      " [   22     0    17     9   115     3    43    10    38]\n",
      " [    3    33    14     3    10   150     8    10   115]\n",
      " [   32     9   127    26    34    13   278    24   208]\n",
      " [   27     4    27   249     1     1    12   747   239]\n",
      " [   13    28    67     8     3    30    42    16 42552]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_features)\n",
    "a =classification_report(tests_gold_labels,prediction)\n",
    "print(confusion_matrix(tests_gold_labels, prediction))\n",
    "#param_grid = {'C':[1,10,100,1000]}\n",
    "param_grid = { 'C':[1,100,1000],'max_iter':[1000,3000]}\n",
    "grid = GridSearchCV(LinearSVC(),param_grid,refit = True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bd36ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.84      0.78      0.81      1837\n",
      "      B-MISC       0.81      0.70      0.75       922\n",
      "       B-ORG       0.65      0.66      0.65      1341\n",
      "       B-PER       0.77      0.71      0.74      1842\n",
      "       I-LOC       0.63      0.45      0.52       257\n",
      "      I-MISC       0.67      0.43      0.53       346\n",
      "       I-ORG       0.59      0.37      0.45       751\n",
      "       I-PER       0.59      0.57      0.58      1307\n",
      "           O       0.97      1.00      0.98     42759\n",
      "\n",
      "    accuracy                           0.94     51362\n",
      "   macro avg       0.72      0.63      0.67     51362\n",
      "weighted avg       0.93      0.94      0.93     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66356dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'loss', 'max_iter', 'multi_class', 'penalty', 'random_state', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearSVC().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4209cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END .................................C=1, max_iter=1000; total time= 1.2min\n",
      "[CV] END .................................C=1, max_iter=1000; total time= 1.0min\n",
      "[CV] END .................................C=1, max_iter=1000; total time= 1.1min\n",
      "[CV] END .................................C=1, max_iter=1000; total time= 1.1min\n",
      "[CV] END .................................C=1, max_iter=1000; total time= 1.1min\n",
      "[CV] END .................................C=1, max_iter=3000; total time= 1.1min\n",
      "[CV] END .................................C=1, max_iter=3000; total time=  59.0s\n",
      "[CV] END .................................C=1, max_iter=3000; total time= 1.1min\n",
      "[CV] END .................................C=1, max_iter=3000; total time= 1.2min\n",
      "[CV] END .................................C=1, max_iter=3000; total time= 1.1min\n",
      "[CV] END ...............................C=100, max_iter=1000; total time= 4.7min\n",
      "[CV] END ...............................C=100, max_iter=1000; total time= 4.7min\n",
      "[CV] END ...............................C=100, max_iter=1000; total time= 5.0min\n",
      "[CV] END ...............................C=100, max_iter=1000; total time= 5.1min\n",
      "[CV] END ...............................C=100, max_iter=1000; total time= 4.7min\n",
      "[CV] END ...............................C=100, max_iter=3000; total time=10.3min\n",
      "[CV] END ...............................C=100, max_iter=3000; total time=10.5min\n",
      "[CV] END ...............................C=100, max_iter=3000; total time=13.5min\n",
      "[CV] END ...............................C=100, max_iter=3000; total time=13.3min\n",
      "[CV] END ...............................C=100, max_iter=3000; total time=13.3min\n",
      "[CV] END ..............................C=1000, max_iter=1000; total time= 6.6min\n",
      "[CV] END ..............................C=1000, max_iter=1000; total time= 6.5min\n",
      "[CV] END ..............................C=1000, max_iter=1000; total time= 7.4min\n",
      "[CV] END ..............................C=1000, max_iter=1000; total time= 7.4min\n",
      "[CV] END ..............................C=1000, max_iter=1000; total time= 6.7min\n",
      "[CV] END ..............................C=1000, max_iter=3000; total time=17.4min\n",
      "[CV] END ..............................C=1000, max_iter=3000; total time=16.8min\n",
      "[CV] END ..............................C=1000, max_iter=3000; total time=19.2min\n",
      "[CV] END ..............................C=1000, max_iter=3000; total time=19.5min\n",
      "[CV] END ..............................C=1000, max_iter=3000; total time=17.5min\n",
      "Best Parameters \n",
      ":  {'C': 1, 'max_iter': 1000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.84      0.78      0.81      1837\n",
      "      B-MISC       0.81      0.70      0.75       922\n",
      "       B-ORG       0.65      0.66      0.65      1341\n",
      "       B-PER       0.77      0.71      0.74      1842\n",
      "       I-LOC       0.63      0.45      0.52       257\n",
      "      I-MISC       0.67      0.43      0.53       346\n",
      "       I-ORG       0.59      0.37      0.45       751\n",
      "       I-PER       0.59      0.57      0.58      1307\n",
      "           O       0.97      1.00      0.98     42759\n",
      "\n",
      "    accuracy                           0.94     51362\n",
      "   macro avg       0.72      0.63      0.67     51362\n",
      "weighted avg       0.93      0.94      0.93     51362\n",
      "\n",
      "[[ 1430    22   177    32    10     3    19    36   108]\n",
      " [   41   644    36    27     1    15    11    11   136]\n",
      " [   99    51   886    32     6     7    59    52   149]\n",
      " [   26     7    17  1303     3     3     2   365   116]\n",
      " [   22     0    17     9   115     3    43    10    38]\n",
      " [    3    33    14     3    10   150     8    10   115]\n",
      " [   32     9   127    26    34    13   278    24   208]\n",
      " [   27     4    27   249     1     1    12   747   239]\n",
      " [   13    28    67     8     3    30    42    16 42552]]\n"
     ]
    }
   ],
   "source": [
    "grid.fit(training_features[:100000], gold_labels[:100000])\n",
    "print(\"Best Parameters \\n: \",grid.best_params_)\n",
    "gold_labels\n",
    "predic = grid.predict(test_features)\n",
    "print(classification_report(tests_gold_labels,predic))\n",
    "print(confusion_matrix(tests_gold_labels, predic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9eef8",
   "metadata": {},
   "source": [
    "# NB - LG - SVM with Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f8b5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingfile = \"conll2003.train_extracted_features.conll\"\n",
    "inputfile = \"conll2003.dev_extracted_features.conll\"\n",
    "#outputfile = \"output.conll2003_features\"\n",
    "#language_model = \"/Users/orbaytopal/Desktop/VUAI/Master/Machine learning NLP/ma-ml4nlp-labs-main/data/GoogleNews-vectors-negative300.bin\"\n",
    "    \n",
    "selected_features = [\"token\",\"pos\",\"tag\",\"previous\",\"latter\",\"capitals\",\"stemm\",\"lemma\"]\n",
    "training_features, gold_labels = extract_features_and_selected_labels(trainingfile, selected_features)\n",
    "test_features, test_gold_labels = extract_features_and_selected_labels(inputfile,selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9319631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NB\n",
    "vec = DictVectorizer()\n",
    "model_NB = MultinomialNB()\n",
    "features_vectorized = vec.fit_transform(training_features)\n",
    "model_NB.fit(features_vectorized, gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80b4acd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.80      0.87      0.83      1837\n",
      "      B-MISC       0.86      0.75      0.80       922\n",
      "       B-ORG       0.79      0.73      0.76      1341\n",
      "       B-PER       0.93      0.75      0.83      1842\n",
      "       I-LOC       0.92      0.50      0.65       257\n",
      "      I-MISC       0.92      0.45      0.61       346\n",
      "       I-ORG       0.81      0.60      0.69       751\n",
      "       I-PER       0.93      0.71      0.81      1307\n",
      "           O       0.97      0.99      0.98     42759\n",
      "\n",
      "    accuracy                           0.95     51362\n",
      "   macro avg       0.88      0.71      0.77     51362\n",
      "weighted avg       0.95      0.95      0.95     51362\n",
      "\n",
      "[[ 1595    11    72    12     2     0     1     3   141]\n",
      " [   48   694    22     9     0     2     2     2   143]\n",
      " [   95    18   975    21     0     0    23     3   206]\n",
      " [   74     1    16  1373     0     0     2    22   354]\n",
      " [   37     2     5     0   128     0    46    14    25]\n",
      " [    7    37    15     1     6   157    11    13    99]\n",
      " [   49     6    49     5     3     5   447     7   180]\n",
      " [   11     2     8    46     0     0     4   934   302]\n",
      " [   86    32    65    17     0     7    17     9 42526]]\n"
     ]
    }
   ],
   "source": [
    "features = vec.transform(test_features)\n",
    "prediction_NB = model_NB.predict(features)\n",
    "print(classification_report(tests_gold_labels,prediction_NB))\n",
    "print(confusion_matrix(tests_gold_labels, prediction_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "816bb213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.91      0.86      0.88      1837\n",
      "      B-MISC       0.93      0.77      0.84       922\n",
      "       B-ORG       0.85      0.76      0.81      1341\n",
      "       B-PER       0.88      0.89      0.88      1842\n",
      "       I-LOC       0.91      0.78      0.84       257\n",
      "      I-MISC       0.90      0.65      0.76       346\n",
      "       I-ORG       0.86      0.70      0.77       751\n",
      "       I-PER       0.81      0.94      0.87      1307\n",
      "           O       0.99      0.99      0.99     42759\n",
      "\n",
      "    accuracy                           0.97     51362\n",
      "   macro avg       0.89      0.82      0.85     51362\n",
      "weighted avg       0.97      0.97      0.97     51362\n",
      "\n",
      "[[ 1574     8    75    49     3     0     9    11   108]\n",
      " [   20   712    34    28     0     4     4    14   106]\n",
      " [   59    14  1024    81     0     2    26    38    97]\n",
      " [   39     2    14  1640     0     1     4    39   103]\n",
      " [    6     0     2     0   200     3     8    19    19]\n",
      " [    7    15     4     1     5   226     3    23    62]\n",
      " [   22     5    17     3    11     6   528    51   108]\n",
      " [    4     1     4    15     0     2     8  1235    38]\n",
      " [    7     9    29    51     0     6    23    89 42545]]\n"
     ]
    }
   ],
   "source": [
    "#LR\n",
    "model_LG = LogisticRegression(max_iter=10000)\n",
    "vec = DictVectorizer()\n",
    "features_vectorized = vec.fit_transform(training_features)\n",
    "model_LG.fit(features_vectorized, gold_labels)\n",
    "features = vec.transform(test_features)\n",
    "prediction_LG = model_LG.predict(features)\n",
    "\n",
    "print(classification_report(tests_gold_labels,prediction_LG))\n",
    "print(confusion_matrix(tests_gold_labels, prediction_LG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "059392f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.91      0.89      0.90      1837\n",
      "      B-MISC       0.92      0.81      0.87       922\n",
      "       B-ORG       0.87      0.79      0.83      1341\n",
      "       B-PER       0.89      0.90      0.89      1842\n",
      "       I-LOC       0.90      0.81      0.85       257\n",
      "      I-MISC       0.83      0.68      0.75       346\n",
      "       I-ORG       0.85      0.74      0.79       751\n",
      "       I-PER       0.86      0.95      0.91      1307\n",
      "           O       0.99      1.00      0.99     42759\n",
      "         ner       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97     51362\n",
      "   macro avg       0.80      0.76      0.78     51362\n",
      "weighted avg       0.97      0.97      0.97     51362\n",
      "\n",
      "[[ 1639     6    73    44     7     1     7     6    54     0]\n",
      " [   18   750    26    19     0     4     3     8    94     0]\n",
      " [   52    16  1062    80     0     3    32    20    76     0]\n",
      " [   32     4    13  1661     0     0     4    43    85     0]\n",
      " [    8     0     0     0   209     6    11    11    12     0]\n",
      " [    4    23     4     3     0   236     4    15    57     0]\n",
      " [   27     5    18     8    13    13   556    40    71     0]\n",
      " [    5     0     2    12     1     2     5  1248    32     0]\n",
      " [   11     7    25    43     3    18    32    55 42564     1]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "model_SVM = LinearSVC(max_iter=10000)\n",
    "vec = DictVectorizer()\n",
    "features_vectorized = vec.fit_transform(training_features)\n",
    "model_SVM.fit(features_vectorized, gold_labels)\n",
    "features = vec.transform(test_features)\n",
    "prediction_SVM = model_SVM.predict(features)\n",
    "\n",
    "print(classification_report(tests_gold_labels,prediction_SVM))\n",
    "print(confusion_matrix(tests_gold_labels, prediction_SVM))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
