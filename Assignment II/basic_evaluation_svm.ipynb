{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158bb4e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this assignment is to create a basic program that provides an overview of basic evaluation metrics (in particular, precision, recall, f-score and a confusion matrix) from documents provided in the conll format. You will need to implement the calculations for precision, recall and f-score yourself (i.e. do not use an existing module that spits them out). Make sure that your code can handle the situation where there are no true positives for a specific class.\n",
    "\n",
    "This notebook provides functions for reading in conll structures with pandas and proposes a structure for calculating your evaluation metrics and producing the confusion matrix. Feel free to adjust the proposed structure if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6819beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict, Counter\n",
    "# module for verifying output\n",
    "from nose.tools import assert_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c64523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(inputfile, annotationcolumn,column_name,delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    \n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter, on_bad_lines='skip',names=column_name)\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa2f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations_mini(inputfile, annotationcolumn,delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    \n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter, on_bad_lines='skip')\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1237f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Computational',\n",
       " 'Lexicology',\n",
       " 'and',\n",
       " 'Terminology',\n",
       " 'Lab',\n",
       " 'headed',\n",
       " 'by',\n",
       " 'Piek',\n",
       " 'Vossen',\n",
       " 'offers',\n",
       " 'mutliple',\n",
       " 'courses',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_annotations_mini(\"datas/minigold.csv\",\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30fadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldannotations = extract_annotations_mini(\"datas/minigold.csv\",\"gold\")\n",
    "machineannotations = extract_annotations_mini(\"datas/miniout1.csv\",\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0784624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machineannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b261e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 10, 'B-ORG': 2, 'I-ORG': 2, 'B-PER': 1, 'I-PER': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in machineannotations:\n",
    "    results[i] = machineannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ff1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 8, 'B-ORG': 2, 'I-ORG': 3, 'B-PER': 1, 'I-PER': 1, 'B-MISC': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in goldannotations:\n",
    "    results[i] = goldannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5932b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_counts(goldannotations, machineannotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts for each predicted and gold class pair\n",
    "    '''\n",
    "    evaluation_counts = defaultdict(Counter)\n",
    "    \n",
    "    evaluation_counts['O']['O'] = 0\n",
    "    evaluation_counts['O']['B-ORG'] = 0\n",
    "    evaluation_counts['O']['I-ORG'] = 0\n",
    "    evaluation_counts['O']['B-PER'] = 0\n",
    "    evaluation_counts['O']['I-PER'] = 0\n",
    "    evaluation_counts['O']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['B-ORG']['O'] = 0\n",
    "    evaluation_counts['B-ORG']['B-ORG'] = 0\n",
    "    evaluation_counts['B-ORG']['I-ORG'] = 0\n",
    "    evaluation_counts['B-ORG']['B-PER'] = 0\n",
    "    evaluation_counts['B-ORG']['I-PER'] = 0\n",
    "    evaluation_counts['B-ORG']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['I-ORG']['O'] = 0\n",
    "    evaluation_counts['I-ORG']['B-ORG'] = 0\n",
    "    evaluation_counts['I-ORG']['I-ORG'] = 0\n",
    "    evaluation_counts['I-ORG']['B-PER'] = 0\n",
    "    evaluation_counts['I-ORG']['I-PER'] = 0\n",
    "    evaluation_counts['I-ORG']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['B-PER']['O'] = 0\n",
    "    evaluation_counts['B-PER']['B-ORG'] = 0\n",
    "    evaluation_counts['B-PER']['I-ORG'] = 0\n",
    "    evaluation_counts['B-PER']['B-PER'] = 0\n",
    "    evaluation_counts['B-PER']['I-PER'] = 0\n",
    "    evaluation_counts['B-PER']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['I-PER']['O'] = 0\n",
    "    evaluation_counts['I-PER']['B-ORG'] = 0\n",
    "    evaluation_counts['I-PER']['I-ORG'] = 0\n",
    "    evaluation_counts['I-PER']['B-PER'] = 0\n",
    "    evaluation_counts['I-PER']['I-PER'] = 0\n",
    "    evaluation_counts['I-PER']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['B-MISC']['O'] = 0\n",
    "    evaluation_counts['B-MISC']['B-ORG'] = 0\n",
    "    evaluation_counts['B-MISC']['I-ORG'] = 0\n",
    "    evaluation_counts['B-MISC']['B-PER'] = 0\n",
    "    evaluation_counts['B-MISC']['I-PER'] = 0\n",
    "    evaluation_counts['B-MISC']['B-MISC'] = 0\n",
    "\n",
    "    for gold_annotation, machine_annotation in zip(goldannotations, machineannotations):\n",
    "        evaluation_counts[gold_annotation][machine_annotation] += 1\n",
    "        \n",
    "    return evaluation_counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc6e1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'O': Counter({'O': 8,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'B-ORG': Counter({'O': 0,\n",
       "                      'B-ORG': 2,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'I-ORG': Counter({'O': 1,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 2,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'B-PER': Counter({'O': 0,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 1,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'I-PER': Counter({'O': 0,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 1,\n",
       "                      'B-MISC': 0}),\n",
       "             'B-MISC': Counter({'O': 1,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0})})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "evaluation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "650d063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_confusion_matrix(evaluation_counts):\n",
    "    '''\n",
    "    Read in the evaluation counts and provide a confusion matrix for each class\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :prints out a confusion matrix\n",
    "    '''\n",
    "    \n",
    "    confusion_matrix = pd.DataFrame.from_dict({i: evaluation_counts[i] for i in evaluation_counts.keys()}, orient='index')\n",
    "    #confusion_matrix = confusion_matrix.reindex(sorted(confusion_matrix.columns), axis=1)\n",
    "    #confusion_matrix = confusion_matrix.reindex(sorted(confusion_matrix.columns), axis=0)\n",
    "    confusion_matrix = confusion_matrix.fillna(0)\n",
    "    #confusion_matrix = confusion_matrix.round(0).astype(int)\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ec9a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        O  B-ORG  I-ORG  B-PER  I-PER  B-MISC\n",
       "O       8      0      0      0      0       0\n",
       "B-ORG   0      2      0      0      0       0\n",
       "I-ORG   1      0      2      0      0       0\n",
       "B-PER   0      0      0      1      0       0\n",
       "I-PER   0      0      0      0      1       0\n",
       "B-MISC  1      0      0      0      0       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cac5ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param calculate_true_false: a tuple from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type calculate_true_false: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "        \n",
    "    # recall = TP / (TP+FN)\n",
    "    # precision = TP / (TP+FP)\n",
    "    # f1_score = (2*precision*recall) / (precision+recall)\n",
    "    # accuracy =  (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    conf_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "    \n",
    "    sum_of_rows = conf_matrix.sum(axis=1)\n",
    "    sum_of_columns = conf_matrix.sum(axis=0)\n",
    "    #total_sum = conf_matrix.sum()\n",
    "    \n",
    "    # initialize the lists\n",
    "    accuracy = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1_score = []\n",
    "    \n",
    "    for i in range(len(conf_matrix)):\n",
    "        precision.append((conf_matrix.iloc[i, i] / sum_of_columns[i]))\n",
    "        recall.append(conf_matrix.iloc[i, i] / sum_of_rows[i])\n",
    "        f1_score.append((2* (precision[i] * recall[i])) / (precision[i] + recall[i]))\n",
    "\n",
    "    return precision,recall,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88459dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\tB-PER\tI-ORG\tI-PER\tO\n",
      "[0.8, 1.0, 1.0, 1.0, 1.0, nan]\n",
      "[1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]\n",
      "[0.888888888888889, 1.0, 0.8, 1.0, 1.0, nan]\n"
     ]
    }
   ],
   "source": [
    "precision,recall,f1_score = calculate_precision_recall_fscore(evaluation_counts)\n",
    "print(\"B-ORG\tB-PER\tI-ORG\tI-PER\tO\")\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e06e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carry_out_evaluation(gold_annotations, systemfile, systemcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param gold_annotations: list of gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param delimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    system_annotations = extract_annotations_mini(systemfile, systemcolumn, delimiter)\n",
    "    evaluation_counts = obtain_counts(gold_annotations, system_annotations)\n",
    "    provide_confusion_matrix(evaluation_counts)\n",
    "    evaluation_outcome = calculate_precision_recall_fscore(evaluation_counts)\n",
    "    \n",
    "    return evaluation_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b67fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_output_tables(evaluations):\n",
    "    '''\n",
    "    Create tables based on the evaluation of various systems\n",
    "    \n",
    "    :param evaluations: the outcome of evaluating one or more systems\n",
    "    '''\n",
    "    #https:stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "    evaluations_pddf = pd.DataFrame.from_dict({i: evaluation_counts[i] for i in evaluation_counts.keys()}, orient='index')\n",
    "    \n",
    "    print(evaluations_pddf)\n",
    "    print(evaluations_pddf.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1613d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(goldfile, goldcolumn, systems):\n",
    "    '''\n",
    "    Carry out standard evaluation for one or more system outputs\n",
    "    \n",
    "    :param goldfile: path to file with goldstandard\n",
    "    :param goldcolumn: indicator of column in gold file where gold labels can be found\n",
    "    :param systems: required information to find and process system output\n",
    "    :type goldfile: string\n",
    "    :type goldcolumn: integer\n",
    "    :type systems: list (providing file name, information on tab with system output and system name for each element)\n",
    "    \n",
    "    :returns the evaluations for all systems\n",
    "    '''\n",
    "    evaluations = {}\n",
    "    #not specifying delimiters here, since it corresponds to the default ('\\t')\n",
    "    gold_annotations = extract_annotations_mini(goldfile, goldcolumn)\n",
    "    for system in systems:\n",
    "        sys_evaluation = carry_out_evaluation(gold_annotations, system[0], system[1])\n",
    "        evaluations[system[2]] = sys_evaluation\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "673a0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_evaluation_value(system, class_label, value_name, evaluations):\n",
    "    '''\n",
    "    Return the outcome of a specific value of the evaluation\n",
    "    \n",
    "    :param system: the name of the system\n",
    "    :param class_label: the name of the class for which the value should be returned\n",
    "    :param value_name: the name of the score that is returned\n",
    "    :param evaluations: the overview of evaluations\n",
    "    \n",
    "    :returns the requested value\n",
    "    '''\n",
    "    return evaluations[system][class_label][value_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01906635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_information(system_information):\n",
    "    '''\n",
    "    Takes system information in the form that it is passed on through sys.argv or via a settingsfile\n",
    "    and returns a list of elements specifying all the needed information on each system output file to carry out the evaluation.\n",
    "    \n",
    "    :param system_information is the input as from a commandline or an input file\n",
    "    '''\n",
    "    # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    systems_list = [system_information[i:i + 3] for i in range(0, len(system_information), 3)]\n",
    "    return systems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c18c04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(label_gold, label_model):\n",
    "    eval_counts = obtain_counts(label_gold, label_model)\n",
    "    confusion_matrix = provide_confusion_matrix(eval_counts)\n",
    "    precision,recall,f1_score = calculate_precision_recall_fscore(eval_counts)\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc7d3b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        O  B-ORG  I-ORG  B-PER  I-PER  B-MISC\n",
      "O       8      0      0      0      0       0\n",
      "B-ORG   0      2      0      0      0       0\n",
      "I-ORG   1      0      2      0      0       0\n",
      "B-PER   0      0      0      1      0       0\n",
      "I-PER   0      0      0      0      1       0\n",
      "B-MISC  1      0      0      0      0       0\n",
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "{} &  O &  B-ORG &  I-ORG &  B-PER &  I-PER &  B-MISC \\\\\n",
      "\\midrule\n",
      "O      &  8 &      0 &      0 &      0 &      0 &       0 \\\\\n",
      "B-ORG  &  0 &      2 &      0 &      0 &      0 &       0 \\\\\n",
      "I-ORG  &  1 &      0 &      2 &      0 &      0 &       0 \\\\\n",
      "B-PER  &  0 &      0 &      0 &      1 &      0 &       0 \\\\\n",
      "I-PER  &  0 &      0 &      0 &      0 &      1 &       0 \\\\\n",
      "B-MISC &  1 &      0 &      0 &      0 &      0 &       0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(my_args=None):\n",
    "    '''\n",
    "    A main function. This does not make sense for a notebook, but it is here as an example.\n",
    "    sys.argv is a very lightweight way of passing arguments from the commandline to a script.\n",
    "    '''\n",
    "    if my_args is None:\n",
    "        my_args = sys.argv\n",
    "    \n",
    "    system_info = create_system_information(my_args[2:])\n",
    "    evaluations = run_evaluations(my_args[0], my_args[1], system_info)\n",
    "    provide_output_tables(evaluations)\n",
    "    #check_eval = identify_evaluation_value('system1', 'O', 'f-score', evaluations)\n",
    "    #if it does not work correctly, this assert statement will indicate that\n",
    "    #assert_equal(\"%.3f\" % check_eval,\"0.889\")\n",
    "    \n",
    "\n",
    "# these can come from the commandline using sys.argv for instance\n",
    "my_args = ['datas/conll2003.results.conll','O','datas/conll2003.dev.conll','O','system1']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6cc53",
   "metadata": {},
   "source": [
    "#some additional tests\n",
    "\n",
    "test_args = ['../../data/minigold.csv','gold','../../data/miniout2.csv','NER','system2']\n",
    "system_info = create_system_information(test_args[2:])\n",
    "evaluations = run_evaluations(test_args[0], test_args[1], system_info)\n",
    "test_eval = identify_evaluation_value('system2', 'I-ORG', 'f-score', evaluations)\n",
    "assert_equal(\"%.3f\" % test_eval,\"0.571\")\n",
    "test_eval2 = identify_evaluation_value('system2', 'I-PER', 'precision', evaluations)\n",
    "assert_equal(\"%.3f\" % test_eval2,\"0.500\")\n",
    "test_eval3 = identify_evaluation_value('system2', 'I-ORG', 'recall', evaluations)\n",
    "assert_equal(\"%.3f\" % test_eval3,\"0.667\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d668a0",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Evaluation\n",
    "\n",
    "## Linear Regression Model with Validation and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62122b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_information(filename,targetcolumn:str,predictedcolum:str,column_names:list):\n",
    "\n",
    "    goldannotations = extract_annotations(filename,targetcolumn,column_names)\n",
    "    machineannotations = extract_annotations(filename,predictedcolum,column_names)\n",
    "    evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "    confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "    print(\"\\n Confusion matrix : \\n\", confusion_matrix)\n",
    "\n",
    "    evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "    precision,recall,f1_score = calculate_precision_recall_fscore(evaluation_counts)\n",
    "\n",
    "    print(\"\\n Precision : \\n\", precision)\n",
    "    print(\"\\n Recall : \\n\",recall)\n",
    "    print(\"\\n F1 Score : \\n\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c08e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data Model\n",
    "print_information(\"outputfile\",\"gold\",\"predicted\",[\"token\",\"tag1\",\"tag2\",\"gold\",\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data Model\n",
    "print_information(\"devfile_model.csv\",\"gold\",\"predicted\",[\"token\",\"tag1\",\"tag2\",\"gold\",\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67043a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion matrix : \n",
      "             O  B-ORG  I-ORG  B-PER  I-PER  B-MISC    NaN  B-LOC  I-MISC  I-LOC\n",
      "O       41861     45     53     11     29      47  640.0     17      42     18\n",
      "B-ORG     172    706     85     42     85      50    0.0    152      16     33\n",
      "I-ORG     192     95    259     27     37      20    0.0     51      11     59\n",
      "B-PER     129     32      6   1178    447      14    0.0     32       3      1\n",
      "I-PER     236     33      8    214    772      13    0.0     14       9      4\n",
      "B-MISC    135     63     28     16     15     577    0.0     47      36      5\n",
      "B-LOC     120    153     32     20     40      29    0.0   1409       6     28\n",
      "I-MISC    100     14      6      3     18      31    0.0      7     154     13\n",
      "I-LOC      44     10     12      6     10       2    0.0     22      16    135\n",
      "\n",
      " Precision : \n",
      " [0.973760729488939, 0.6133796698523023, 0.5296523517382413, 0.7765326301911668, 0.5313145216792843, 0.7369093231162197, 0.0, 0.003997715591090805, 0.05460750853242321]\n",
      "\n",
      " Recall : \n",
      " [0.9789069990412272, 0.5264727815063386, 0.3448735019973369, 0.6395222584147665, 0.59247889485802, 0.6258134490238612, 0.0, 0.02023121387283237, 0.0622568093385214]\n",
      "\n",
      " F1 Score : \n",
      " [0.9763270827502565, 0.56661316211878, 0.417741935483871, 0.7013992259601072, 0.5602322206095791, 0.67683284457478, nan, 0.006676204101096805, 0.05818181818181818]\n"
     ]
    }
   ],
   "source": [
    "print_information(\"svm_output.embedded.conll2003_embeddings\",\"gold\",\"predicted\",[\"token\",\"tag1\",\"tag2\",\"gold\",\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_information_without_label(filename,targetcolumn:str,predictedcolum:str,column_names:list):\n",
    "\n",
    "    goldannotations = extract_annotations(filename,targetcolumn,column_names)\n",
    "    machineannotations = extract_annotations(filename,predictedcolum,column_names)\n",
    "    evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "    confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "    print(\"\\n Confusion matrix : \\n\", confusion_matrix)\n",
    "\n",
    "    evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "    precision,recall,f1_score = calculate_precision_recall_fscore(evaluation_counts)\n",
    "\n",
    "    print(\"\\n Precision : \\n\", precision)\n",
    "    print(\"\\n Recall : \\n\",recall)\n",
    "    print(\"\\n F1 Score : \\n\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de6ed11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./models/output.SVM.conll2003_features\", sep=\"\\t\", on_bad_lines='skip')\n",
    "data.columns = [0, \"LEICESTERSHIRE\", \"NNP\", \"B-NP\",\"B-ORG\",\"-\",\"TAKE\",\"1\",\"leicestershir\",\"LEICESTERSHIRE\",\"B-ORG\"]\n",
    "#data.pop\n",
    "a =data.head(2)\n",
    "data.head()\n",
    "data.to_csv(\"proper_output.SVM.conll2003_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97fadb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_information(\"proper_output.SVM.conll2003_features\",\"gold\",\"predicted\",[\"token\",\"tag1\",\"tag2\",\"gold\",\"previous\",\"latter\",\"capitals\",\"stemm\",\"lemma\",\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa49ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
