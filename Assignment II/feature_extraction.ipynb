{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b2f255",
   "metadata": {},
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4485baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('omw-1.4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1c74ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(conll_file_path, delimiter='\\t'):\n",
    "    \n",
    "    '''\n",
    "    This function returns a csv reader of conll file\n",
    "    \n",
    "    :param conll_file_path: the path to the conll file\n",
    "    :param delimiter: the name of the column in which the target annotation is provided\n",
    "    :type conll_file_path: string\n",
    "    :type delimiter: string\n",
    "    \n",
    "    :returns: provides a structured representation of the data in the conll file.\n",
    "    '''\n",
    "    \n",
    "    conll_file = open(conll_file_path, 'r')\n",
    "    c_file = csv.reader(conll_file, delimiter=delimiter)\n",
    "    \n",
    "    return c_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "04528446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_feature(conll_file):\n",
    "    \n",
    "    '''\n",
    "    This function reads and tokenize all words in the data \n",
    "    \n",
    "    :param conll_file: the path to the conll file\n",
    "    :type conll_file: string\n",
    "    \n",
    "    :returns: provides list with tokenized words\n",
    "    '''\n",
    "    \n",
    "    conll = read_conll_file(conll_file)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for index,row in enumerate(conll):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if len(row) > 0:\n",
    "            token = row[0]\n",
    "            tokens.append(token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ce83ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalization_feature(tokens):\n",
    "    \n",
    "    '''\n",
    "    This function checks whether tokens are capitalized or not\n",
    "    :param tokens: the list of tokenized data\n",
    "    :type tokens: list\n",
    "    \n",
    "    :returns: provides list which 0 (not capitalized) and 1 (capitalized) for tokens\n",
    "    '''\n",
    "\n",
    "    capitals = []\n",
    "            \n",
    "    for t in tokens:\n",
    "        if t.isupper():\n",
    "            capitals.append(1)\n",
    "        else:\n",
    "            capitals.append(0)\n",
    "        \n",
    "    return capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1ae95e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_feature(tokens):\n",
    "    \n",
    "    '''\n",
    "    This function applies lemmatization to tokens\n",
    "    \n",
    "    :returns: a list with lemmatized tokens\n",
    "    '''\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "            \n",
    "    for t in tokens:\n",
    "        lemmas.append(wnl.lemmatize(t))\n",
    "        \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5641885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_latter_token(tokens):\n",
    "    \n",
    "    '''\n",
    "    This function extracts previous and latter tokens and appends them to a list\n",
    "    \n",
    "    :returns: two lists with previous and latter tokens\n",
    "    '''\n",
    "\n",
    "    previous_tokens = []\n",
    "    latter_tokens = []\n",
    "    \n",
    "    previous = ' '\n",
    "    latter = ' '\n",
    "\n",
    "    for index, token in enumerate(tokens):\n",
    "        \n",
    "        if index > 0:\n",
    "            previous = tokens[index - 1]\n",
    "        previous_tokens.append(previous)\n",
    "        \n",
    "        if index < (len(tokens)-1):\n",
    "            latter = tokens[index + 1]\n",
    "        latter_tokens.append(latter)\n",
    "    \n",
    "    return previous_tokens,latter_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "95b11fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_feature(tokens):\n",
    "    \n",
    "    '''\n",
    "    This function applies stemming to tokens\n",
    "    \n",
    "    :returns: a list with stemmized tokens\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    stemm = []\n",
    "            \n",
    "    for t in tokens:\n",
    "        stemm.append(ps.stem(t))\n",
    "        \n",
    "    return stemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "70a81a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    args = sys.argv\n",
    "    trainingfile = args[1]\n",
    "    testfile = args[2]\n",
    "    \n",
    "    training_data = pd.read_csv(\"datas/conll2003.train.conll\", sep='\\t')\n",
    "    training_data.columns = [\"token\",\"pos\",\"tag\",\"ner\"]\n",
    "    test_data = pd.read_csv(\"datas/conll2003.dev.conll\", sep='\\t')\n",
    "    test_data.columns = [\"token\",\"pos\",\"tag\",\"gold\"]\n",
    "    \n",
    "    tokens_train = tokenization_feature(\"datas/conll2003.train.conll\")\n",
    "    previous_tokens_train,latter_tokens_train = prev_latter_token(tokens_train)\n",
    "    capitalized_tokens_train = capitalization_feature(tokens_train)\n",
    "    stemmed_tokens_train = stemming_feature(tokens_train)\n",
    "    lemmatized_tokens_train = lemmatization_feature(tokens_train)\n",
    "    \n",
    "    \n",
    "    training_data['previous'] = previous_tokens_train\n",
    "    training_data['latter'] = latter_tokens_train\n",
    "    training_data['capitals'] = capitalized_tokens_train\n",
    "    training_data['stemm'] = stemmed_tokens_train\n",
    "    training_data['lemma'] = lemmatized_tokens_train\n",
    "    \n",
    "    training_data.to_csv(\"conll2003.train_extracted_features.conll\", sep='\\t', index=False)\n",
    "    \n",
    "    tokens_test = tokenization_feature(\"datas/conll2003.dev.conll\")\n",
    "    previous_tokens_test,latter_tokens_test = prev_latter_token(tokens_test)\n",
    "    capitalized_tokens_test = capitalization_feature(tokens_test)\n",
    "    stemmed_tokens_test = stemming_feature(tokens_test)\n",
    "    lemmatized_tokens_test = lemmatization_feature(tokens_test)\n",
    "    \n",
    "    test_data['previous'] = previous_tokens_test\n",
    "    test_data['latter'] = latter_tokens_test\n",
    "    test_data['capitals'] = capitalized_tokens_test\n",
    "    test_data['stemm'] = stemmed_tokens_test\n",
    "    test_data['lemma'] = lemmatized_tokens_test\n",
    "    \n",
    "    test_data.to_csv(\"conll2003.dev_extracted_features.conll\",sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "02c3fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd928a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
