{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9a645c",
   "metadata": {},
   "source": [
    "# NER Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc921994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    ### This code was partially inspired by code included in the HLT course, obtained from https://github.com/cltl/ma-hlt-labs/, accessed in May 2020.\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #check for cases where empty lines mark sentence boundaries (which some conll files do).\n",
    "        if len(row) > 3:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a5922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_features(inputfile,word_embedding_model):\n",
    "    '''\n",
    "    This function extracts features from embeddings\n",
    "    \n",
    "    :param inputfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    '''\n",
    "    ### This code was partially inspired by code included in the HLT course, obtained from https://github.com/cltl/ma-hlt-labs/, accessed in May 2020.\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(inputfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        #check for cases where empty lines mark sentence boundaries (which some conll files do).\n",
    "        if len(row) > 3:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indexes = {'token': 0, 'pos': 1, 'tag': 2, 'previous': 4, 'latter': 5, 'capitals': 6,'stemm':7,'lemma':8}\n",
    "\n",
    "def extract_features_and_selected_labels(trainingfile, selected_features):\n",
    "    '''\n",
    "    Extract features and gold labels from a preprocessed file with the training data and return them as lists\n",
    "    \n",
    "    :param trainingfile: path to training file\n",
    "    :param selected_features: list of features that will be used to train the model\n",
    "    \n",
    "    :type trainingfile: string\n",
    "    :type selected_features: list\n",
    "    \n",
    "    :return features: features as a list of dictionaries\n",
    "    :return gold_labels: list of gold labels\n",
    "    '''\n",
    "    features = []\n",
    "    gold_labels = []\n",
    "    \n",
    "    conllinput = open(trainingfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        feature_value = {}\n",
    "        # Only extract the selected features\n",
    "        for feature_name in selected_features:\n",
    "            row_index = feature_indexes.get(feature_name)\n",
    "            feature_value[feature_name] = row[row_index]\n",
    "        features.append(feature_value)\n",
    "        \n",
    "        # Gold is in the third column\n",
    "        gold_labels.append(row[3])\n",
    "                \n",
    "    return features, gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43d5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(trainingfile):\n",
    "    \n",
    "    data = []\n",
    "    targets = []\n",
    "    # TIP: recall that you can find information on how to integrate features here:\n",
    "    # https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    with open(trainingfile, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                token = components[0]\n",
    "                feature_dict = {'token':token}\n",
    "                data.append(feature_dict)\n",
    "                #gold is in the last column\n",
    "                targets.append(components[-1])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a44315",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_index = {'token': 0, 'pos': 1, 'tag': 2, 'previous': 4, 'latter': 5, 'capitals': 6,'stemm':7,'lemma':8}\n",
    "\n",
    "def extract_features(testfile, selected_features):\n",
    "    '''Extract features from a preprocessed file with the test data and return them as a list\n",
    "    \n",
    "    :param trainingfile: path to test file\n",
    "    :param selected_features: list of features that were selected to train the model\n",
    "    \n",
    "    :type testfile: string\n",
    "    :type selected_features: list\n",
    "    \n",
    "    :return features: features as a list of dictionaries'''\n",
    "\n",
    "    features = []\n",
    "    gold_labels = []\n",
    "    \n",
    "    conllinput = open(testfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        feature_value = {}\n",
    "        for feature_name in selected_features:\n",
    "            row_index = feature_to_index.get(feature_name)\n",
    "            feature_value[feature_name] = row[row_index]\n",
    "        features.append(feature_value)\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adea2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = extract_features_and_labels(\"datas/conll2003.train.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44606677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': 'EU'}, {'token': 'rejects'}, {'token': 'German'}, {'token': 'call'}, {'token': 'to'}, {'token': 'boycott'}, {'token': 'British'}, {'token': 'lamb'}, {'token': '.'}, {'token': 'Peter'}]\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0d10da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "print(targets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b741e4f",
   "metadata": {},
   "source": [
    "    def extract_features(inputfile):\n",
    "   \n",
    "    data = []\n",
    "    with open(inputfile, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                token = components[0]\n",
    "                feature_dict = {'token':token}\n",
    "                data.append(feature_dict)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc6f8aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': 'token', 'pos': 'pos', 'tag': 'tag', 'previous': 'previous', 'latter': 'latter', 'capitals': 'capitals', 'stemm': 'stemm', 'lemma': 'lemma'}, {'token': 'rejects', 'pos': 'VBZ', 'tag': 'B-VP', 'previous': ' ', 'latter': 'German', 'capitals': '0', 'stemm': 'reject', 'lemma': 'reject'}, {'token': 'German', 'pos': 'JJ', 'tag': 'B-NP', 'previous': 'rejects', 'latter': 'call', 'capitals': '0', 'stemm': 'german', 'lemma': 'German'}, {'token': 'call', 'pos': 'NN', 'tag': 'I-NP', 'previous': 'German', 'latter': 'to', 'capitals': '0', 'stemm': 'call', 'lemma': 'call'}, {'token': 'to', 'pos': 'TO', 'tag': 'B-VP', 'previous': 'call', 'latter': 'boycott', 'capitals': '0', 'stemm': 'to', 'lemma': 'to'}, {'token': 'boycott', 'pos': 'VB', 'tag': 'I-VP', 'previous': 'to', 'latter': 'British', 'capitals': '0', 'stemm': 'boycott', 'lemma': 'boycott'}, {'token': 'British', 'pos': 'JJ', 'tag': 'B-NP', 'previous': 'boycott', 'latter': 'lamb', 'capitals': '0', 'stemm': 'british', 'lemma': 'British'}, {'token': 'lamb', 'pos': 'NN', 'tag': 'I-NP', 'previous': 'British', 'latter': '.', 'capitals': '0', 'stemm': 'lamb', 'lemma': 'lamb'}, {'token': '.', 'pos': '.', 'tag': 'O', 'previous': 'lamb', 'latter': 'Peter', 'capitals': '0', 'stemm': '.', 'lemma': '.'}, {'token': 'Peter', 'pos': 'NNP', 'tag': 'B-NP', 'previous': '.', 'latter': 'Blackburn', 'capitals': '0', 'stemm': 'peter', 'lemma': 'Peter'}]\n"
     ]
    }
   ],
   "source": [
    "data = extract_features(\"./conll2003.train_extracted_features.conll\",[\"token\",\"pos\",\"tag\",\"previous\",\"latter\",\"capitals\",\"stemm\",\"lemma\"])\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efaa1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(train_features, train_targets, modelname):\n",
    "    \n",
    "    '''Create a classifier and train it with vectorized features and corresponding gold labels\n",
    "    \n",
    "    input train_features: features to be transformed into vectors\n",
    "    input train_labels: gold labels corresponding to features\n",
    "    input modelname: name of the model that will be trained\n",
    "    \n",
    "    output model: trained classifier\n",
    "    output vec: DictVectorizer'''\n",
    "   \n",
    "    if modelname ==  'logreg':\n",
    "        # TIP: you may need to solve this: https://stackoverflow.com/questions/61814494/what-is-this-warning-convergencewarning-lbfgs-failed-to-converge-status-1\n",
    "        model = LogisticRegression(max_iter=10000)\n",
    "    if modelname == 'NB':\n",
    "        model = MultinomialNB()\n",
    "    if modelname == 'SVM':\n",
    "        model = LinearSVC(max_iter=10000)\n",
    "        \n",
    "    vec = DictVectorizer()\n",
    "    \n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model.fit(features_vectorized, train_targets)\n",
    "    \n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f894d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_embeddings(train_features, train_labels):\n",
    "    '''\n",
    "    Create an SVM classifier and train it with vectorized features and corresponding gold labels\n",
    "    \n",
    "    input train_features: features to be transformed into vectors\n",
    "    input train_labels: gold labels corresponding to features\n",
    "    \n",
    "    output model: trained classifier\n",
    "    '''\n",
    "\n",
    "    model = LinearSVC(max_iter=10000)\n",
    "    model.fit(train_features, train_labels)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93cea902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, inputdata, outputfile,selected_features):\n",
    "  \n",
    "    # Extracting features from input data\n",
    "    features = extract_features(inputdata,selected_features)\n",
    "    features = vec.transform(features)\n",
    "    \n",
    "    # Making prediction\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    # Writing the results\n",
    "    outfile = open(outputfile, 'w')\n",
    "    counter = 0\n",
    "    for line in open(inputdata, 'r'):\n",
    "        if len(line.rstrip('\\n').split()) > 0:\n",
    "            outfile.write(line.rstrip('\\n') + '\\t' + predictions[counter] + '\\n')\n",
    "            counter += 1\n",
    "    outfile.close()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "251898a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data_embeddings(model, inputdata, outputfile, word_embedding_model):\n",
    "    '''\n",
    "    This function creates a classifier for making predictions embedded data\n",
    "    \n",
    "    input model: classifier that will make predictions\n",
    "    input inputdata: path to input data\n",
    "    input outputfile: path to output file, where the predictions for each feature will be written\n",
    "    input word_embedding_model : embedding model\n",
    "    '''\n",
    "    # extracting features\n",
    "    features = extract_embeddings_features(inputdata,word_embedding_model)\n",
    "    \n",
    "    # making predictions with extracted features\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    # Write results to an outputfile\n",
    "    outfile = open(outputfile, 'w')\n",
    "    counter = 0\n",
    "    for line in open(inputdata, 'r'):\n",
    "        if len(line.rstrip('\\n').split()) > 0:\n",
    "            outfile.write(line.rstrip('\\n') + '\\t' + predictions[counter] + '\\n')\n",
    "            counter += 1\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "201eeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(system_type,argv=None):\n",
    "    \n",
    "    #a very basic way for picking up commandline arguments\n",
    "    if argv is None:\n",
    "        argv = sys.argv    \n",
    "    \n",
    "    # LR - NB - SVM with extracted features\n",
    "    trainingfile = \"conll2003.train_extracted_features.conll\"\n",
    "    inputfile = \"conll2003.dev_extracted_features.conll\"\n",
    "    outputfile = \"output.conll2003_features\"\n",
    "    language_model = \"./models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "    \n",
    "    # SVM with embeddings\n",
    "    trainingfile_svm = \"datas/conll2003.train.conll\"\n",
    "    inputfile_svm = \"datas/conll2003.dev.conll\"\n",
    "    outputfile_svm = \"svm_output.conll2003_embeddings\"\n",
    "    \n",
    "    if system_type == \"with_features\":\n",
    "        \n",
    "        # selecting features to train the model\n",
    "        selected_features = [\"token\",\"pos\",\"tag\",\"previous\",\"latter\",\"capitals\",\"stemm\",\"lemma\"]\n",
    "    \n",
    "        # getting the selected training features and gold labels\n",
    "        training_features, gold_labels = extract_features_and_selected_labels(trainingfile,selected_features)\n",
    "    \n",
    "        # Training three different models with the features, \n",
    "        # Classifying the data and writing the result to new conll files        \n",
    "        for modelname in ['logreg', 'NB', 'SVM']:\n",
    "        \n",
    "            ml_model, vec = create_classifier(training_features, gold_labels, modelname)\n",
    "            classify_data(ml_model, vec, inputfile, outputfile.replace('.conll','.' + modelname + '.conll'),selected_features)\n",
    "        \n",
    "            dataframe = pd.read_table(outputfile.replace('.conll','.' + modelname + '.conll'))\n",
    "            dataframe = dataframe.set_axis([*dataframe.columns[:-1], 'NER2'], axis=1, inplace=False)\n",
    "            dataframe.to_csv(outputfile.replace('.conll','.' + modelname + '.conll'), sep='\\t')\n",
    "            \n",
    "    elif system_type == \"word_embeddings\":\n",
    "\n",
    "        # creating a language model\n",
    "        language_model = gensim.models.KeyedVectors.load_word2vec_format(language_model, binary=True)\n",
    "\n",
    "        # extracting the features and gold label\n",
    "        training_features, gold_labels = extract_embeddings_as_features_and_gold(trainingfile_svm, language_model)\n",
    "\n",
    "        # creating the classification model\n",
    "        ml_model = create_classifier_embeddings(training_features[:50000], gold_labels[:50000])\n",
    "        classify_data_embeddings(ml_model, inputfile_svm, outputfile_svm.replace('.conll','.embedded.conll'), language_model)\n",
    "\n",
    "        data_frame = pd.read_table(outputfile_svm.replace('.conll','.embedded.conll'))\n",
    "        data_frame = data_frame.set_axis([*data_frame.columns[:-1], 'NER2'], axis=1, inplace=False)\n",
    "        data_frame.to_csv(outputfile_svm.replace('.conll','.embedded_last_50000.conll'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84607a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(system_type=\"word_embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
