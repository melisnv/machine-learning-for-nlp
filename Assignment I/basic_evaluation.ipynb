{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158bb4e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this assignment is to create a basic program that provides an overview of basic evaluation metrics (in particular, precision, recall, f-score and a confusion matrix) from documents provided in the conll format. You will need to implement the calculations for precision, recall and f-score yourself (i.e. do not use an existing module that spits them out). Make sure that your code can handle the situation where there are no true positives for a specific class.\n",
    "\n",
    "This notebook provides functions for reading in conll structures with pandas and proposes a structure for calculating your evaluation metrics and producing the confusion matrix. Feel free to adjust the proposed structure if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6819beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict, Counter\n",
    "# module for verifying output\n",
    "from nose.tools import assert_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c64523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(inputfile, annotationcolumn,column_name,delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    \n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter, on_bad_lines='skip',names=column_name)\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa2f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations_mini(inputfile, annotationcolumn,delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    \n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter, on_bad_lines='skip')\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1237f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Computational',\n",
       " 'Lexicology',\n",
       " 'and',\n",
       " 'Terminology',\n",
       " 'Lab',\n",
       " 'headed',\n",
       " 'by',\n",
       " 'Piek',\n",
       " 'Vossen',\n",
       " 'offers',\n",
       " 'mutliple',\n",
       " 'courses',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_annotations_mini(\"datas/minigold.csv\",\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30fadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldannotations = extract_annotations_mini(\"datas/minigold.csv\",\"gold\")\n",
    "machineannotations = extract_annotations_mini(\"datas/miniout1.csv\",\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0784624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machineannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b261e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 10, 'B-ORG': 2, 'I-ORG': 2, 'B-PER': 1, 'I-PER': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in machineannotations:\n",
    "    results[i] = machineannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ff1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 8, 'B-ORG': 2, 'I-ORG': 3, 'B-PER': 1, 'I-PER': 1, 'B-MISC': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in goldannotations:\n",
    "    results[i] = goldannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5932b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_counts(goldannotations, machineannotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts for each predicted and gold class pair\n",
    "    '''\n",
    "    evaluation_counts = defaultdict(Counter)\n",
    "    \n",
    "    evaluation_counts['O']['O'] = 0\n",
    "    evaluation_counts['O']['B-ORG'] = 0\n",
    "    evaluation_counts['O']['I-ORG'] = 0\n",
    "    evaluation_counts['O']['B-PER'] = 0\n",
    "    evaluation_counts['O']['I-PER'] = 0\n",
    "    evaluation_counts['O']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['B-ORG']['O'] = 0\n",
    "    evaluation_counts['B-ORG']['B-ORG'] = 0\n",
    "    evaluation_counts['B-ORG']['I-ORG'] = 0\n",
    "    evaluation_counts['B-ORG']['B-PER'] = 0\n",
    "    evaluation_counts['B-ORG']['I-PER'] = 0\n",
    "    evaluation_counts['B-ORG']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['I-ORG']['O'] = 0\n",
    "    evaluation_counts['I-ORG']['B-ORG'] = 0\n",
    "    evaluation_counts['I-ORG']['I-ORG'] = 0\n",
    "    evaluation_counts['I-ORG']['B-PER'] = 0\n",
    "    evaluation_counts['I-ORG']['I-PER'] = 0\n",
    "    evaluation_counts['I-ORG']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['B-PER']['O'] = 0\n",
    "    evaluation_counts['B-PER']['B-ORG'] = 0\n",
    "    evaluation_counts['B-PER']['I-ORG'] = 0\n",
    "    evaluation_counts['B-PER']['B-PER'] = 0\n",
    "    evaluation_counts['B-PER']['I-PER'] = 0\n",
    "    evaluation_counts['B-PER']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['I-PER']['O'] = 0\n",
    "    evaluation_counts['I-PER']['B-ORG'] = 0\n",
    "    evaluation_counts['I-PER']['I-ORG'] = 0\n",
    "    evaluation_counts['I-PER']['B-PER'] = 0\n",
    "    evaluation_counts['I-PER']['I-PER'] = 0\n",
    "    evaluation_counts['I-PER']['B-MISC'] = 0\n",
    "\n",
    "    evaluation_counts['B-MISC']['O'] = 0\n",
    "    evaluation_counts['B-MISC']['B-ORG'] = 0\n",
    "    evaluation_counts['B-MISC']['I-ORG'] = 0\n",
    "    evaluation_counts['B-MISC']['B-PER'] = 0\n",
    "    evaluation_counts['B-MISC']['I-PER'] = 0\n",
    "    evaluation_counts['B-MISC']['B-MISC'] = 0\n",
    "\n",
    "    for gold_annotation, machine_annotation in zip(goldannotations, machineannotations):\n",
    "        evaluation_counts[gold_annotation][machine_annotation] += 1\n",
    "        \n",
    "    return evaluation_counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc6e1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'O': Counter({'O': 8,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'B-ORG': Counter({'O': 0,\n",
       "                      'B-ORG': 2,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'I-ORG': Counter({'O': 1,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 2,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'B-PER': Counter({'O': 0,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 1,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0}),\n",
       "             'I-PER': Counter({'O': 0,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 1,\n",
       "                      'B-MISC': 0}),\n",
       "             'B-MISC': Counter({'O': 1,\n",
       "                      'B-ORG': 0,\n",
       "                      'I-ORG': 0,\n",
       "                      'B-PER': 0,\n",
       "                      'I-PER': 0,\n",
       "                      'B-MISC': 0})})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "evaluation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650d063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_confusion_matrix(evaluation_counts):\n",
    "    '''\n",
    "    Read in the evaluation counts and provide a confusion matrix for each class\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :prints out a confusion matrix\n",
    "    '''\n",
    "    \n",
    "    confusion_matrix = pd.DataFrame.from_dict({i: evaluation_counts[i] for i in evaluation_counts.keys()}, orient='index')\n",
    "    #confusion_matrix = confusion_matrix.reindex(sorted(confusion_matrix.columns), axis=1)\n",
    "    #confusion_matrix = confusion_matrix.reindex(sorted(confusion_matrix.columns), axis=0)\n",
    "    confusion_matrix = confusion_matrix.fillna(0)\n",
    "    #confusion_matrix = confusion_matrix.round(0).astype(int)\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ec9a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-MISC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        O  B-ORG  I-ORG  B-PER  I-PER  B-MISC\n",
       "O       8      0      0      0      0       0\n",
       "B-ORG   0      2      0      0      0       0\n",
       "I-ORG   1      0      2      0      0       0\n",
       "B-PER   0      0      0      1      0       0\n",
       "I-PER   0      0      0      0      1       0\n",
       "B-MISC  1      0      0      0      0       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac5ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param calculate_true_false: a tuple from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type calculate_true_false: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "        \n",
    "    # recall = TP / (TP+FN)\n",
    "    # precision = TP / (TP+FP)\n",
    "    # f1_score = (2*precision*recall) / (precision+recall)\n",
    "    # accuracy =  (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    conf_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "    \n",
    "    sum_of_rows = conf_matrix.sum(axis=1)\n",
    "    sum_of_columns = conf_matrix.sum(axis=0)\n",
    "    #total_sum = conf_matrix.sum()\n",
    "    \n",
    "    # initialize the lists\n",
    "    accuracy = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1_score = []\n",
    "    \n",
    "    for i in range(len(conf_matrix)):\n",
    "        precision.append((conf_matrix.iloc[i, i] / sum_of_columns[i]))\n",
    "        recall.append(conf_matrix.iloc[i, i] / sum_of_rows[i])\n",
    "        f1_score.append((2* (precision[i] * recall[i])) / (precision[i] + recall[i]))\n",
    "\n",
    "    return precision,recall,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88459dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\tB-PER\tI-ORG\tI-PER\tO\n",
      "[0.8, 1.0, 1.0, 1.0, 1.0, nan]\n",
      "[1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]\n",
      "[0.888888888888889, 1.0, 0.8, 1.0, 1.0, nan]\n"
     ]
    }
   ],
   "source": [
    "precision,recall,f1_score = calculate_precision_recall_fscore(evaluation_counts)\n",
    "print(\"B-ORG\tB-PER\tI-ORG\tI-PER\tO\")\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e06e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carry_out_evaluation(gold_annotations, systemfile, systemcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param gold_annotations: list of gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param delimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    system_annotations = extract_annotations_mini(systemfile, systemcolumn, delimiter)\n",
    "    evaluation_counts = obtain_counts(gold_annotations, system_annotations)\n",
    "    provide_confusion_matrix(evaluation_counts)\n",
    "    evaluation_outcome = calculate_precision_recall_fscore(evaluation_counts)\n",
    "    \n",
    "    return evaluation_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07b67fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_output_tables(evaluations):\n",
    "    '''\n",
    "    Create tables based on the evaluation of various systems\n",
    "    \n",
    "    :param evaluations: the outcome of evaluating one or more systems\n",
    "    '''\n",
    "    #https:stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "    evaluations_pddf = pd.DataFrame.from_dict({(i,j): evaluations[i][j]\n",
    "                                              for i in evaluations.keys()\n",
    "                                              for j in evaluations[i].keys()},\n",
    "                                             orient='index')\n",
    "    print(evaluations_pddf)\n",
    "    print(evaluations_pddf.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1613d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(goldfile, goldcolumn, systems):\n",
    "    '''\n",
    "    Carry out standard evaluation for one or more system outputs\n",
    "    \n",
    "    :param goldfile: path to file with goldstandard\n",
    "    :param goldcolumn: indicator of column in gold file where gold labels can be found\n",
    "    :param systems: required information to find and process system output\n",
    "    :type goldfile: string\n",
    "    :type goldcolumn: integer\n",
    "    :type systems: list (providing file name, information on tab with system output and system name for each element)\n",
    "    \n",
    "    :returns the evaluations for all systems\n",
    "    '''\n",
    "    evaluations = {}\n",
    "    #not specifying delimiters here, since it corresponds to the default ('\\t')\n",
    "    gold_annotations = extract_annotations_mini(goldfile, goldcolumn)\n",
    "    for system in systems:\n",
    "        sys_evaluation = carry_out_evaluation(gold_annotations, system[0], system[1])\n",
    "        evaluations[system[2]] = sys_evaluation\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "673a0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_evaluation_value(system, class_label, value_name, evaluations):\n",
    "    '''\n",
    "    Return the outcome of a specific value of the evaluation\n",
    "    \n",
    "    :param system: the name of the system\n",
    "    :param class_label: the name of the class for which the value should be returned\n",
    "    :param value_name: the name of the score that is returned\n",
    "    :param evaluations: the overview of evaluations\n",
    "    \n",
    "    :returns the requested value\n",
    "    '''\n",
    "    return evaluations[system][class_label][value_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01906635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_information(system_information):\n",
    "    '''\n",
    "    Takes system information in the form that it is passed on through sys.argv or via a settingsfile\n",
    "    and returns a list of elements specifying all the needed information on each system output file to carry out the evaluation.\n",
    "    \n",
    "    :param system_information is the input as from a commandline or an input file\n",
    "    '''\n",
    "    # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    systems_list = [system_information[i:i + 3] for i in range(0, len(system_information), 3)]\n",
    "    return systems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c18c04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(label_gold, label_model):\n",
    "    eval_counts = obtain_counts(label_gold, label_model)\n",
    "    confusion_matrix = provide_confusion_matrix(eval_counts)\n",
    "    precision,recall,f1_score = calculate_precision_recall_fscore(eval_counts)\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc7d3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(my_args=None):\n",
    "    '''\n",
    "    A main function. This does not make sense for a notebook, but it is here as an example.\n",
    "    sys.argv is a very lightweight way of passing arguments from the commandline to a script.\n",
    "    '''\n",
    "    \n",
    "    if my_args is None:\n",
    "        my_args = sys.argv\n",
    "    \n",
    "    system_info = create_system_information(my_args[2:])\n",
    "    evaluations = run_evaluations(my_args[0], my_args[1], system_info)\n",
    "    provide_output_tables(evaluations)\n",
    "    check_eval = identify_evaluation_value('system1', 'O', 'f-score', evaluations)\n",
    "    #if it does not work correctly, this assert statement will indicate that\n",
    "    assert_equal(\"%.3f\" % check_eval,\"0.889\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d668a0",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Evaluation\n",
    "\n",
    "## Linear Regression Model with Validation and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62122b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_information(filename,targetcolumn:str,predictedcolum:str,column_names:list):\n",
    "\n",
    "    goldannotations = extract_annotations(filename,targetcolumn,column_names)\n",
    "    machineannotations = extract_annotations(filename,predictedcolum,column_names)\n",
    "    evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "    confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "    print(\"\\n Confusion matrix : \\n\", confusion_matrix)\n",
    "\n",
    "    evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "    precision,recall,f1_score = calculate_precision_recall_fscore(evaluation_counts)\n",
    "\n",
    "    print(\"\\n Precision : \\n\", precision)\n",
    "    print(\"\\n Recall : \\n\",recall)\n",
    "    print(\"\\n F1 Score : \\n\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2385bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion matrix : \n",
      "              O  B-ORG  I-ORG  B-PER  I-PER  B-MISC     NaN  I-MISC  B-LOC  \\\n",
      "O       167258     21     38      3      6      15  2178.0    37.0     18   \n",
      "B-ORG     1394   4191    133     22     21     138     0.0     5.0    385   \n",
      "I-ORG     1494    130   1703     23     15      38     0.0    12.0    151   \n",
      "B-PER     1915     25      7   4347    290       2     0.0     0.0     10   \n",
      "I-PER     2165     34     10    544   1730       3     0.0     6.0     33   \n",
      "B-MISC     653     70     12     19      1    2478     0.0    56.0    144   \n",
      "B-LOC     1101    329     45     11      7      35     0.0     2.0   5578   \n",
      "I-MISC     477     19     27      1      8      87     0.0   515.0     11   \n",
      "I-LOC      323      8     71     14     13       5     0.0     3.0     48   \n",
      "\n",
      "        I-LOC  \n",
      "O           4  \n",
      "B-ORG      32  \n",
      "I-ORG     138  \n",
      "B-PER       4  \n",
      "I-PER       3  \n",
      "B-MISC      5  \n",
      "B-LOC      32  \n",
      "I-MISC     10  \n",
      "I-LOC     672  \n",
      "\n",
      " Precision : \n",
      " [0.9461364407738432, 0.8682411435674332, 0.832355816226784, 0.8721910112359551, 0.8273553323768532, 0.8846840414137808, 0.0, 0.809748427672956, 0.007525870178739417]\n",
      "\n",
      " Recall : \n",
      " [0.9863189800563752, 0.6630280018984338, 0.4597732181425486, 0.6586363636363637, 0.3820671378091873, 0.7207678883071553, 0.0, 0.4458874458874459, 0.04148660328435609]\n",
      "\n",
      " F1 Score : \n",
      " [0.965809942314022, 0.7518837459634016, 0.5923478260869566, 0.7505179558011049, 0.522737573651609, 0.7943580702035582, nan, 0.5750977107761027, 0.012740544127405442]\n"
     ]
    }
   ],
   "source": [
    "# Train Data Model\n",
    "print_information(\"outputfile\",\"gold\",\"predicted\",[\"token\",\"tag1\",\"tag2\",\"gold\",\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61042a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion matrix : \n",
      "             O  B-ORG  I-ORG  B-PER  I-PER  B-MISC    NaN  I-MISC  B-LOC  I-LOC\n",
      "O       42083      4     11    0.0      1      10  640.0    10.0      3    1.0\n",
      "B-ORG     479    690     38    5.0     14      23    0.0     3.0     78   11.0\n",
      "I-ORG     342     47    263    4.0      5      11    0.0     5.0     36   38.0\n",
      "B-PER     843      2      1  873.0    104       3    0.0     0.0     16    0.0\n",
      "I-PER     895      5      1  102.0    292       2    0.0     0.0      6    0.0\n",
      "B-MISC    241     14      2    8.0      1     603    0.0    12.0     41    0.0\n",
      "B-LOC     395    101      6    4.0      4      15    0.0     0.0   1305    7.0\n",
      "I-MISC    150      7      2    2.0      4      27    0.0   145.0      2    7.0\n",
      "I-LOC      69      1     13    0.0      6       2    0.0     3.0     13  150.0\n",
      "\n",
      " Precision : \n",
      " [0.924962085412225, 0.7921928817451206, 0.7804154302670623, 0.874749498997996, 0.6774941995359629, 0.8663793103448276, 0.0, 0.8146067415730337, 0.008666666666666666]\n",
      "\n",
      " Recall : \n",
      " [0.9840984028248719, 0.5145413870246085, 0.35019973368841545, 0.4739413680781759, 0.22409823484267075, 0.6540130151843818, 0.0, 0.4190751445086705, 0.05058365758754864]\n",
      "\n",
      " F1 Score : \n",
      " [0.9536143213233628, 0.6238698010849909, 0.4834558823529412, 0.6147887323943662, 0.33679354094579006, 0.7453646477132262, nan, 0.5534351145038168, 0.014797951052931132]\n"
     ]
    }
   ],
   "source": [
    "# Validation Data Model\n",
    "print_information(\"devfile_model.csv\",\"gold\",\"predicted\",[\"token\",\"tag1\",\"tag2\",\"gold\",\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f07c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
