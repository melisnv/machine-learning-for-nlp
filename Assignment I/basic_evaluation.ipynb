{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158bb4e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this assignment is to create a basic program that provides an overview of basic evaluation metrics (in particular, precision, recall, f-score and a confusion matrix) from documents provided in the conll format. You will need to implement the calculations for precision, recall and f-score yourself (i.e. do not use an existing module that spits them out). Make sure that your code can handle the situation where there are no true positives for a specific class.\n",
    "\n",
    "This notebook provides functions for reading in conll structures with pandas and proposes a structure for calculating your evaluation metrics and producing the confusion matrix. Feel free to adjust the proposed structure if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6819beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict, Counter\n",
    "# module for verifying output\n",
    "from nose.tools import assert_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c64523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(inputfile, annotationcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter, on_bad_lines='skip')\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1237f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Computational',\n",
       " 'Lexicology',\n",
       " 'and',\n",
       " 'Terminology',\n",
       " 'Lab',\n",
       " 'headed',\n",
       " 'by',\n",
       " 'Piek',\n",
       " 'Vossen',\n",
       " 'offers',\n",
       " 'mutliple',\n",
       " 'courses',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_annotations(\"datas/minigold.csv\",\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30fadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldannotations = extract_annotations(\"datas/minigold.csv\",\"gold\")\n",
    "machineannotations = extract_annotations(\"datas/miniout1.csv\",\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3bca2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0784624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machineannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d63e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 10, 'B-ORG': 2, 'I-ORG': 2, 'B-PER': 1, 'I-PER': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in machineannotations:\n",
    "    results[i] = machineannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eea7749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 8, 'B-ORG': 2, 'I-ORG': 3, 'B-PER': 1, 'I-PER': 1, 'B-MISC': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in goldannotations:\n",
    "    results[i] = goldannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82bd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_counts(goldannotations, machineannotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts for each predicted and gold class pair\n",
    "    '''\n",
    "    \n",
    "    # TIP on how to get the counts for each class\n",
    "    # https://stackoverflow.com/questions/49393683/how-to-count-items-in-a-nested-dictionary, last accessed 22.10.2020\n",
    "    evaluation_counts = defaultdict(Counter)\n",
    "        \n",
    "    for i, j in zip(goldannotations, machineannotations):\n",
    "        evaluation_counts[i][j] += 1\n",
    "\n",
    "    \n",
    "    return evaluation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b52113d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'O': Counter({'O': 8}),\n",
       "             'B-ORG': Counter({'B-ORG': 2}),\n",
       "             'I-ORG': Counter({'I-ORG': 2, 'O': 1}),\n",
       "             'B-PER': Counter({'B-PER': 1}),\n",
       "             'I-PER': Counter({'I-PER': 1}),\n",
       "             'B-MISC': Counter({'O': 1})})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "evaluation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab674344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'I-ORG': 2, 'O': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_counts['I-ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efaf570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  8\n",
      "TP:  10\n",
      "TP:  12\n",
      "FP:  1\n",
      "FN:  1\n",
      "TP:  13\n",
      "TP:  14\n",
      "FP:  2\n"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for k,v in evaluation_counts.items():\n",
    "    for i,j in v.items():\n",
    "        #print(\"i:\",i)\n",
    "        if i == k:\n",
    "            TP += int(j) \n",
    "            print(\"TP: \",TP)\n",
    "        elif i != k:\n",
    "            FP += int(j)\n",
    "            print(\"FP: \",FP)\n",
    "            #print(\"not equal:\",i[0])\n",
    "            if len(v) >= 2 and i != k:\n",
    "                FN += int(j)\n",
    "                print(\"FN: \",FN)\n",
    "                #print(\"i,j:\",i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb79608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_true_false(evaluation_counts):\n",
    "    \n",
    "    '''\n",
    "    Calculates true positives, false positives and false negatives for each class and return them in a tuple\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns true positives, false positives and false negatives of each class in a tuple\n",
    "    '''\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for k,v in evaluation_counts.items(): # i = ('O', 8), v = Counter({'O': 8}), k = O \n",
    "        for i,j in v.items():\n",
    "            if i == k:\n",
    "                TP += int(j) \n",
    "            elif i != k:\n",
    "                FP += int(j)\n",
    "                if len(v) >= 2 and i != k:\n",
    "                    FN += int(j)\n",
    "                    \n",
    "    return TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8fb55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0 2 1\n"
     ]
    }
   ],
   "source": [
    "TP,TN,FP,FN = calculate_true_false(evaluation_counts)\n",
    "print(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c33f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param calculate_true_false: a tuple from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type calculate_true_false: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "    \n",
    "    # TIP: you may want to write a separate function that provides an overview of true positives, false positives and false negatives\n",
    "    #      for each class based on the outcome of obtain counts\n",
    "    \n",
    "    TP,TN,FP,FN = calculate_true_false(evaluation_counts)\n",
    "    \n",
    "    recall = TP / (TP+FN)\n",
    "    precision = TP / (TP+FP)\n",
    "    f1_score = (2*precision*recall) / (precision+recall) \n",
    "    \n",
    "    return precision,recall,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d099bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.875\n",
      "recall :  0.9333333333333333\n",
      "f1_score :  0.9032258064516129\n"
     ]
    }
   ],
   "source": [
    "precision,recall,f1_score = calculate_precision_recall_fscore(evaluation_counts)\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall : \",recall)\n",
    "print(\"f1_score : \",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b605e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_confusion_matrix(evaluation_counts):\n",
    "    '''\n",
    "    Read in the evaluation counts and provide a confusion matrix for each class\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :prints out a confusion matrix\n",
    "    '''\n",
    "    \n",
    "    # TIP: provide_output_tables does something similar, but those tables are assuming one additional nested layer\n",
    "    # your solution can thus be a simpler version of the one provided in provide_output_tables below\n",
    "    \n",
    "    TP,TN,FP,FN = calculate_true_false(evaluation_counts)\n",
    "    confusion_matrix =  np.array([[TN,FN],[FP,TP]])\n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67f5bbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of a given dataset is:\n",
      "[[ 0  1]\n",
      " [ 2 14]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "print(\"Confusion matrix of a given dataset is:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efb0a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carry_out_evaluation(gold_annotations, systemfile, systemcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param gold_annotations: list of gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param delimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    system_annotations = extract_annotations(systemfile, systemcolumn, delimiter)\n",
    "    evaluation_counts = obtain_counts(gold_annotations, system_annotations)\n",
    "    provide_confusion_matrix(evaluation_counts)\n",
    "    evaluation_outcome = calculate_precision_recall_fscore(evaluation_counts)\n",
    "    \n",
    "    return evaluation_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0ebf6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.875, 0.9333333333333333, 0.9032258064516129)\n"
     ]
    }
   ],
   "source": [
    "systemfile = \"datas/miniout1.csv\"\n",
    "evaluation_outcome = carry_out_evaluation(gold_annotations=goldannotations,systemfile=systemfile,systemcolumn=\"NER\")\n",
    "print(evaluation_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "087105a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6875, 0.9166666666666666, 0.7857142857142857)\n"
     ]
    }
   ],
   "source": [
    "systemfile2 = \"datas/miniout2.csv\"\n",
    "evaluation_outcome = carry_out_evaluation(gold_annotations=goldannotations,systemfile=systemfile2,systemcolumn=\"NER\")\n",
    "print(evaluation_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c776ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
