{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158bb4e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this assignment is to create a basic program that provides an overview of basic evaluation metrics (in particular, precision, recall, f-score and a confusion matrix) from documents provided in the conll format. You will need to implement the calculations for precision, recall and f-score yourself (i.e. do not use an existing module that spits them out). Make sure that your code can handle the situation where there are no true positives for a specific class.\n",
    "\n",
    "This notebook provides functions for reading in conll structures with pandas and proposes a structure for calculating your evaluation metrics and producing the confusion matrix. Feel free to adjust the proposed structure if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6819beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict, Counter\n",
    "# module for verifying output\n",
    "from nose.tools import assert_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c64523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(inputfile, annotationcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter, on_bad_lines='skip')\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1237f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Computational',\n",
       " 'Lexicology',\n",
       " 'and',\n",
       " 'Terminology',\n",
       " 'Lab',\n",
       " 'headed',\n",
       " 'by',\n",
       " 'Piek',\n",
       " 'Vossen',\n",
       " 'offers',\n",
       " 'mutliple',\n",
       " 'courses',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_annotations(\"datas/minigold.csv\",\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30fadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "goldannotations = extract_annotations(\"datas/minigold.csv\",\"gold\")\n",
    "machineannotations = extract_annotations(\"datas/miniout1.csv\",\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3bca2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0784624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machineannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b261e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 10, 'B-ORG': 2, 'I-ORG': 2, 'B-PER': 1, 'I-PER': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in machineannotations:\n",
    "    results[i] = machineannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ff1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 8, 'B-ORG': 2, 'I-ORG': 3, 'B-PER': 1, 'I-PER': 1, 'B-MISC': 1}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in goldannotations:\n",
    "    results[i] = goldannotations.count(i)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb291d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_counts(goldannotations, machineannotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts for each predicted and gold class pair\n",
    "    '''\n",
    "    evaluation_counts = defaultdict(Counter)\n",
    "\n",
    "    for gold_annotation, machine_annotation in zip(goldannotations, machineannotations):\n",
    "        evaluation_counts[gold_annotation][machine_annotation] += 1\n",
    "        \n",
    "    return evaluation_counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc6e1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'O': Counter({'O': 8}),\n",
       "             'B-ORG': Counter({'B-ORG': 2}),\n",
       "             'I-ORG': Counter({'I-ORG': 2, 'O': 1}),\n",
       "             'B-PER': Counter({'B-PER': 1}),\n",
       "             'I-PER': Counter({'I-PER': 1}),\n",
       "             'B-MISC': Counter({'O': 1})})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_counts = obtain_counts(goldannotations,machineannotations)\n",
    "evaluation_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "650d063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_confusion_matrix(evaluation_counts):\n",
    "    '''\n",
    "    Read in the evaluation counts and provide a confusion matrix for each class\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :prints out a confusion matrix\n",
    "    '''\n",
    "    \n",
    "    # TIP: provide_output_tables does something similar, but those tables are assuming one additional nested layer\n",
    "    # your solution can thus be a simpler version of the one provided in provide_output_tables below\n",
    "    \n",
    "    confusion_matrix = pd.DataFrame.from_dict({i: evaluation_counts[i] for i in evaluation_counts.keys()}, orient='index')\n",
    "    confusion_matrix = confusion_matrix.reindex(sorted(confusion_matrix.columns), axis=1)\n",
    "    confusion_matrix = confusion_matrix.reindex(sorted(confusion_matrix.columns), axis=0)\n",
    "    confusion_matrix = confusion_matrix.fillna(0)\n",
    "    confusion_matrix = confusion_matrix.round(0).astype(int)\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f2b5df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-ORG</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       B-ORG  B-PER  I-ORG  I-PER  O\n",
       "B-ORG      2      0      0      0  0\n",
       "B-PER      0      1      0      0  0\n",
       "I-ORG      0      0      2      0  1\n",
       "I-PER      0      0      0      1  0\n",
       "O          0      0      0      0  8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cac5ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param calculate_true_false: a tuple from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type calculate_true_false: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "        \n",
    "    # recall = TP / (TP+FN)\n",
    "    # precision = TP / (TP+FP)\n",
    "    # f1_score = (2*precision*recall) / (precision+recall)\n",
    "    # accuracy =  (TP+TN)/(TP+FP+FN+TN)\n",
    "    \n",
    "    conf_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "    \n",
    "    sum_of_rows = conf_matrix.sum(axis=1)\n",
    "    sum_of_columns = conf_matrix.sum(axis=0)\n",
    "    #total_sum = conf_matrix.sum()\n",
    "    \n",
    "    # initialize the lists\n",
    "    accuracy = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1_score = []\n",
    "    \n",
    "    for i in range(len(conf_matrix)):\n",
    "        precision.append((conf_matrix.iloc[i, i] / sum_of_columns[i]))\n",
    "        recall.append(conf_matrix.iloc[i, i] / sum_of_rows[i])\n",
    "        f1_score.append((2* (precision[i] * recall[i])) / (precision[i] + recall[i]))\n",
    "\n",
    "    return precision,recall,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d01b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG\tB-PER\tI-ORG\tI-PER\tO\n",
      "[1.0, 1.0, 1.0, 1.0, 0.8888888888888888]\n",
      "[1.0, 1.0, 0.6666666666666666, 1.0, 1.0]\n",
      "[1.0, 1.0, 0.8, 1.0, 0.9411764705882353]\n"
     ]
    }
   ],
   "source": [
    "precision,recall,f1_score = calculate_precision_recall_fscore(evaluation_counts)\n",
    "print(\"B-ORG\tB-PER\tI-ORG\tI-PER\tO\")\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e06e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carry_out_evaluation(gold_annotations, systemfile, systemcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param gold_annotations: list of gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param delimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    system_annotations = extract_annotations(systemfile, systemcolumn, delimiter)\n",
    "    evaluation_counts = obtain_counts(gold_annotations, system_annotations)\n",
    "    provide_confusion_matrix(evaluation_counts)\n",
    "    evaluation_outcome = calculate_precision_recall_fscore(evaluation_counts)\n",
    "    \n",
    "    return evaluation_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97858623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_output_tables(evaluations):\n",
    "    '''\n",
    "    Create tables based on the evaluation of various systems\n",
    "    \n",
    "    :param evaluations: the outcome of evaluating one or more systems\n",
    "    '''\n",
    "    #https:stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "    evaluations_pddf = pd.DataFrame.from_dict({(i,j): evaluations[i][j]\n",
    "                                              for i in evaluations.keys()\n",
    "                                              for j in evaluations[i].keys()},\n",
    "                                             orient='index')\n",
    "    print(evaluations_pddf)\n",
    "    print(evaluations_pddf.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03226a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(goldfile, goldcolumn, systems):\n",
    "    '''\n",
    "    Carry out standard evaluation for one or more system outputs\n",
    "    \n",
    "    :param goldfile: path to file with goldstandard\n",
    "    :param goldcolumn: indicator of column in gold file where gold labels can be found\n",
    "    :param systems: required information to find and process system output\n",
    "    :type goldfile: string\n",
    "    :type goldcolumn: integer\n",
    "    :type systems: list (providing file name, information on tab with system output and system name for each element)\n",
    "    \n",
    "    :returns the evaluations for all systems\n",
    "    '''\n",
    "    evaluations = {}\n",
    "    #not specifying delimiters here, since it corresponds to the default ('\\t')\n",
    "    gold_annotations = extract_annotations(goldfile, goldcolumn)\n",
    "    for system in systems:\n",
    "        sys_evaluation = carry_out_evaluation(gold_annotations, system[0], system[1])\n",
    "        evaluations[system[2]] = sys_evaluation\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2259cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_evaluation_value(system, class_label, value_name, evaluations):\n",
    "    '''\n",
    "    Return the outcome of a specific value of the evaluation\n",
    "    \n",
    "    :param system: the name of the system\n",
    "    :param class_label: the name of the class for which the value should be returned\n",
    "    :param value_name: the name of the score that is returned\n",
    "    :param evaluations: the overview of evaluations\n",
    "    \n",
    "    :returns the requested value\n",
    "    '''\n",
    "    return evaluations[system][class_label][value_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b32ec5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_information(system_information):\n",
    "    '''\n",
    "    Takes system information in the form that it is passed on through sys.argv or via a settingsfile\n",
    "    and returns a list of elements specifying all the needed information on each system output file to carry out the evaluation.\n",
    "    \n",
    "    :param system_information is the input as from a commandline or an input file\n",
    "    '''\n",
    "    # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    systems_list = [system_information[i:i + 3] for i in range(0, len(system_information), 3)]\n",
    "    return systems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c87129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(label_gold, label_model):\n",
    "    eval_counts = obtain_counts(label_gold, label_model)\n",
    "    confusion_matrix = provide_confusion_matrix(eval_counts)\n",
    "    precision,recall,f1_score = calculate_precision_recall_fscore(eval_counts)\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee57f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(my_args=None):\n",
    "    '''\n",
    "    A main function. This does not make sense for a notebook, but it is here as an example.\n",
    "    sys.argv is a very lightweight way of passing arguments from the commandline to a script.\n",
    "    '''\n",
    "    \n",
    "    if my_args is None:\n",
    "        my_args = sys.argv\n",
    "    \n",
    "    system_info = create_system_information(my_args[2:])\n",
    "    evaluations = run_evaluations(my_args[0], my_args[1], system_info)\n",
    "    provide_output_tables(evaluations)\n",
    "    check_eval = identify_evaluation_value('system1', 'O', 'f-score', evaluations)\n",
    "    #if it does not work correctly, this assert statement will indicate that\n",
    "    assert_equal(\"%.3f\" % check_eval,\"0.889\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dd14282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_args = ['datas/minigold.csv','gold','datas/miniout1.csv','NER','system1']\n",
    "#main(my_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adb8a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1.0, 1.0, 1.0, 1.0, 0.8888888888888888], [1.0, 1.0, 0.6666666666666666, 1.0, 1.0], [1.0, 1.0, 0.8, 1.0, 0.9411764705882353])\n"
     ]
    }
   ],
   "source": [
    "evaluation_outcome = carry_out_evaluation(gold_annotations=goldannotations,systemfile=\"datas/miniout1.csv\",\n",
    "                                          systemcolumn=\"NER\")\n",
    "print(evaluation_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b86274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1.0, 1.0, 0.8888888888888888], [0.6666666666666666, 1.0, 1.0], [0.8, 1.0, 0.9411764705882353])\n"
     ]
    }
   ],
   "source": [
    "evaluation_outcome = carry_out_evaluation(gold_annotations=goldannotations,systemfile=\"datas/miniout2.csv\",\n",
    "                                          systemcolumn=\"NER\")\n",
    "print(evaluation_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb29d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
