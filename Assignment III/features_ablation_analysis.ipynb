{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a12f3a",
   "metadata": {},
   "source": [
    "# Features Ablation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5048519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import sklearn\n",
    "import csv\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6e9eb",
   "metadata": {},
   "source": [
    "# Step 1 : A Basic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8005081",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = './conll2003.train_extracted_features.conll'\n",
    "testfile = './conll2003.dev_extracted_features.conll'\n",
    "\n",
    "def extract_features_token_only_and_labels(conllfile):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        \n",
    "        # Preprocessing the file so that all rows with instances should contain 6 values,\n",
    "        # the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) > 6:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {'Token': row[0]}\n",
    "            features.append(feature_value)\n",
    "            #The 3rd column provides the gold label (= the correct answer). \n",
    "            labels.append(row[3])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b619e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features and labels:\n",
    "feature_values, labels = extract_features_token_only_and_labels(trainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2414f78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Token': 'token'},\n",
       " {'Token': 'rejects'},\n",
       " {'Token': 'German'},\n",
       " {'Token': 'call'},\n",
       " {'Token': 'to'},\n",
       " {'Token': 'boycott'},\n",
       " {'Token': 'British'},\n",
       " {'Token': 'lamb'},\n",
       " {'Token': '.'},\n",
       " {'Token': 'Peter'},\n",
       " {'Token': 'Blackburn'},\n",
       " {'Token': 'BRUSSELS'},\n",
       " {'Token': '1996-08-22'},\n",
       " {'Token': 'The'},\n",
       " {'Token': 'European'},\n",
       " {'Token': 'Commission'},\n",
       " {'Token': 'said'},\n",
       " {'Token': 'on'},\n",
       " {'Token': 'Thursday'},\n",
       " {'Token': 'it'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e329409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f7b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer_and_classifier(features, labels):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return lr_classifier: a trained LogisticRegression classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "    \n",
    "    vec = DictVectorizer()\n",
    "    # fit creates a mapping between observed feature values and dimensions in a one-hot vector, \n",
    "    # transform represents the current values as a vector \n",
    "    tokens_vectorized = vec.fit_transform(features)\n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(tokens_vectorized, labels)\n",
    "    \n",
    "    return lr_classifier, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c100f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766e77c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='saga')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd596c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DictVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DictVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6851d4",
   "metadata": {},
   "source": [
    "# Step 2: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f747abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_and_gold_labels_token_only(testfile, vectorizer, classifier):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    sparse_feature_reps, goldlabels = extract_features_token_only_and_labels(testfile)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(sparse_feature_reps)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56dff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, goldlabels = get_predicted_and_gold_labels_token_only(testfile, vectorizer, lr_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "130f55a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC',\n",
       "       'O', 'B-LOC', 'B-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e8c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldlabels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b0ef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels, 'Predicted': predictions    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6600c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1305      15    101      4      7       0      6      4    395\n",
      "B-MISC        41     603     14      8      0      12      2      1    241\n",
      "B-ORG         78      23    690      5     11       3     38     14    479\n",
      "B-PER         16       3      2    873      0       0      1    104    843\n",
      "I-LOC         13       2      1      0    150       3     13      6     69\n",
      "I-MISC         2      27      7      2      7     145      2      4    150\n",
      "I-ORG         36      11     47      4     38       5    263      5    342\n",
      "I-PER          6       2      5    102      0       0      1    292    899\n",
      "O              3      10      4      0      1      10     11      1  42718\n",
      "gold           0       0      0      0      0       0      0      0      1\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "089fea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,\n",
    "                        average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,\n",
    "                     average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,\n",
    "                 average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be731ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.7302667260105565 R: 0.49283025204234604 F1: 0.5737103836701725\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73856336",
   "metadata": {},
   "source": [
    "# Step 3: A More Elaborate System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d833955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the column in which each feature is located (note: you can also define headers and use csv.DictReader)\n",
    "feature_to_index = {'token': 0, 'pos': 1,'tag':2,'ner':3, 'previous': 4, 'latter': 5, 'capitals': 6,'stemm':7,'lemma':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "819f3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_gold_labels(conllfile, selected_features):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        \n",
    "        if len(row) > 6:\n",
    "            feature_value = {}\n",
    "            \n",
    "            for feature_name in selected_features:\n",
    "                row_index = feature_to_index.get(feature_name)\n",
    "                feature_value[feature_name] = row[row_index]\n",
    "            \n",
    "            features.append(feature_value)\n",
    "            \n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[3])\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2ab85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "all_features = ['token','previous', 'latter', 'capitals','stemm','lemma']\n",
    "\n",
    "sparse_feature_reps, labels = extract_features_and_gold_labels(trainfile, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b72a2665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'token',\n",
       "  'previous': 'previous',\n",
       "  'latter': 'latter',\n",
       "  'capitals': 'capitals',\n",
       "  'stemm': 'stemm',\n",
       "  'lemma': 'lemma'},\n",
       " {'token': 'rejects',\n",
       "  'previous': ' ',\n",
       "  'latter': 'German',\n",
       "  'capitals': '0',\n",
       "  'stemm': 'reject',\n",
       "  'lemma': 'reject'},\n",
       " {'token': 'German',\n",
       "  'previous': 'rejects',\n",
       "  'latter': 'call',\n",
       "  'capitals': '0',\n",
       "  'stemm': 'german',\n",
       "  'lemma': 'German'},\n",
       " {'token': 'call',\n",
       "  'previous': 'German',\n",
       "  'latter': 'to',\n",
       "  'capitals': '0',\n",
       "  'stemm': 'call',\n",
       "  'lemma': 'call'},\n",
       " {'token': 'to',\n",
       "  'previous': 'call',\n",
       "  'latter': 'boycott',\n",
       "  'capitals': '0',\n",
       "  'stemm': 'to',\n",
       "  'lemma': 'to'},\n",
       " {'token': 'boycott',\n",
       "  'previous': 'to',\n",
       "  'latter': 'British',\n",
       "  'capitals': '0',\n",
       "  'stemm': 'boycott',\n",
       "  'lemma': 'boycott'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_feature_reps[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b642f3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'O', 'B-MISC', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "446002e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_and_gold_labels(testfile, vectorizer, classifier, selected_features):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    features, goldlabels = extract_features_and_gold_labels(testfile, selected_features)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(features)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d5a325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(sparse_feature_reps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4af42c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DictVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DictVectorizer()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ad52776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='saga')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb2ae849",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20ce3fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ner', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f72d219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldlabels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d97c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O  \\\n",
      "Gold                                                                         \n",
      "B-LOC       1536      11     65     13      3       0      3      2    204   \n",
      "B-MISC        17     706     11      5      0       7      1      1    174   \n",
      "B-ORG         36      16    955     20      0       0     13     15    286   \n",
      "B-PER         15       1     11   1270      0       0      4     23    518   \n",
      "I-LOC          6       0      1      0    193       2      3      6     46   \n",
      "I-MISC         1      25      2      0      0     211      0      3    104   \n",
      "I-ORG         28       5     13      4      9       7    485      7    193   \n",
      "I-PER          3       0      3     18      0       0      0    907    376   \n",
      "O              6       4     12      8      0       4      9     24  42691   \n",
      "gold           0       0      0      0      0       0      0      0      0   \n",
      "\n",
      "Predicted  ner  \n",
      "Gold            \n",
      "B-LOC        0  \n",
      "B-MISC       0  \n",
      "B-ORG        0  \n",
      "B-PER        0  \n",
      "I-LOC        0  \n",
      "I-MISC       0  \n",
      "I-ORG        0  \n",
      "I-PER        0  \n",
      "O            0  \n",
      "gold         1  \n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37d38d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.7597343180173812 R: 0.609317205364778 F1: 0.6732806711261443\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0c736",
   "metadata": {},
   "source": [
    "    with tokens only\n",
    "P: 0.7302667260105565 R: 0.49283025204234604 F1: 0.5737103836701725\n",
    "\n",
    "    with all features\n",
    "P: 0.7597343180173812 R: 0.609317205364778 F1: 0.6732806711261443"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c95a7b",
   "metadata": {},
   "source": [
    "# Step 4: Feature Ablation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5a7f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1330       5     60      6      4       0      3     13    416\n",
      "B-MISC        19     655      4      4      0      14      2      1    223\n",
      "B-ORG         54      18    781      9      1       2     54     32    390\n",
      "B-PER         12       1     10    960      0       0      1     83    775\n",
      "I-LOC         17       0      6      0    150       5     13      4     62\n",
      "I-MISC         2      41      1      2      1     166      0      5    128\n",
      "I-ORG         24       7     57      2     10       5    355     16    275\n",
      "I-PER          4       0      4     83      0       0      0    596    620\n",
      "O              2       2     17      2      1       8      4     50  42672\n",
      "gold           0       0      0      0      0       0      0      0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.7768221298070672 R: 0.5528116749098522 F1: 0.6391668575108358\n"
     ]
    }
   ],
   "source": [
    "# example of system with just one additional feature\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "selected_features = ['token','latter', 'capitals']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, selected_features)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c96ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1488      12    125     11      7       0     14    116     64\n",
      "B-MISC        34     662     33     12      0      11     14     45    111\n",
      "B-ORG         85      17    888     15     13       3     68    137    115\n",
      "B-PER         17       3     26   1166      0       0      7    518    105\n",
      "I-LOC         20       2      4      0    169       4     17     25     16\n",
      "I-MISC         2      31     18      1      9     169      7     31     78\n",
      "I-ORG         45       5     64     12     41       5    360     83    136\n",
      "I-PER         13       3     16    120      0       0      7   1108     40\n",
      "O              6      11     68     11      1       9     26    158  42468\n",
      "gold           0       0      0      0      0       0      0      0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.7058240414856609 R: 0.6289568936938837 F1: 0.6532858079231936\n"
     ]
    }
   ],
   "source": [
    "# example of system with just one additional feature\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "selected_features = ['token','pos', 'capitals','stemm']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, selected_features)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1407c9",
   "metadata": {},
   "source": [
    "# Part 2: One-hot versus Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eca4fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# create classifier with caps feature only and print vectorizer, then with token only (but you see less)\n",
    "\n",
    "selected_features = ['capitals']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(trainfile, selected_features)\n",
    "\n",
    "#creating a vectorizing\n",
    "vectorizer = DictVectorizer()\n",
    "#fitting the values to dimensions (creating a mapping) and transforming the current observations according to this mapping\n",
    "capitalization_vectorized = vectorizer.fit_transform(feature_values)\n",
    "print(capitalization_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca6382",
   "metadata": {},
   "source": [
    "# Using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df52ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step takes a while\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin.gz',\n",
    "                                                                       binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19ddcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) > 6:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[3])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "483d2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(trainfile,word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e44f3fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.04174805,  0.20410156, -0.26757812,  0.29882812, -0.11181641,\n",
       "        -0.01470947,  0.16992188, -0.09423828,  0.04785156,  0.05810547,\n",
       "        -0.07128906, -0.13867188,  0.04589844,  0.00604248, -0.15917969,\n",
       "         0.10888672,  0.14648438,  0.0145874 ,  0.08398438, -0.23535156,\n",
       "        -0.13378906, -0.02783203,  0.06982422, -0.22558594,  0.05493164,\n",
       "        -0.19042969, -0.3125    ,  0.04541016,  0.09277344,  0.01342773,\n",
       "        -0.01275635, -0.30664062, -0.07275391,  0.1640625 , -0.00075531,\n",
       "        -0.25976562,  0.28710938,  0.10546875, -0.17382812,  0.09277344,\n",
       "         0.06542969,  0.00534058,  0.2734375 , -0.05688477, -0.01367188,\n",
       "        -0.203125  , -0.00601196,  0.11816406, -0.04980469, -0.22851562,\n",
       "        -0.03808594, -0.04785156, -0.03417969, -0.03979492, -0.33203125,\n",
       "        -0.02612305, -0.3125    ,  0.02172852, -0.09033203, -0.20800781,\n",
       "         0.08740234,  0.21289062,  0.11865234, -0.21386719,  0.01428223,\n",
       "        -0.11767578, -0.265625  , -0.02539062,  0.1640625 , -0.04882812,\n",
       "         0.05688477,  0.21484375,  0.00915527, -0.06298828, -0.02282715,\n",
       "        -0.24023438,  0.17285156, -0.12597656,  0.21875   ,  0.04223633,\n",
       "        -0.01080322, -0.02612305, -0.00665283,  0.00396729, -0.23046875,\n",
       "        -0.23632812,  0.23339844, -0.18457031,  0.08544922,  0.20898438,\n",
       "         0.04663086, -0.00720215, -0.37695312, -0.11035156,  0.09521484,\n",
       "         0.13183594, -0.14355469,  0.17382812, -0.34570312, -0.07519531,\n",
       "        -0.09521484, -0.24414062,  0.07324219, -0.11865234, -0.13964844,\n",
       "        -0.00860596,  0.09228516, -0.04589844,  0.21777344, -0.06689453,\n",
       "        -0.15234375, -0.3359375 ,  0.15527344,  0.265625  ,  0.109375  ,\n",
       "         0.18066406,  0.25      , -0.17773438,  0.05541992,  0.09375   ,\n",
       "         0.22167969,  0.02124023, -0.10058594,  0.12109375,  0.02758789,\n",
       "         0.17480469,  0.22753906,  0.17871094,  0.09667969, -0.37890625,\n",
       "        -0.03417969, -0.00312805, -0.203125  , -0.11035156,  0.00506592,\n",
       "        -0.13769531, -0.09472656,  0.1015625 , -0.12060547,  0.14160156,\n",
       "        -0.10107422, -0.18066406, -0.0100708 , -0.12695312, -0.02429199,\n",
       "         0.06298828, -0.08496094, -0.2890625 , -0.1015625 , -0.17871094,\n",
       "        -0.01501465,  0.18457031, -0.08105469, -0.05786133,  0.16601562,\n",
       "        -0.15136719, -0.03930664,  0.03491211, -0.09326172, -0.06079102,\n",
       "         0.07617188, -0.06396484,  0.09912109, -0.21679688, -0.11328125,\n",
       "        -0.14648438,  0.03320312, -0.07324219, -0.42382812, -0.03393555,\n",
       "        -0.01989746, -0.03466797,  0.14648438, -0.16210938, -0.00634766,\n",
       "        -0.11083984,  0.17089844, -0.08056641,  0.33789062,  0.07666016,\n",
       "        -0.04150391, -0.06835938, -0.18359375,  0.19335938, -0.08642578,\n",
       "        -0.08544922,  0.0859375 , -0.21582031, -0.11669922,  0.06884766,\n",
       "        -0.33007812,  0.11425781, -0.17480469, -0.10449219, -0.19433594,\n",
       "        -0.15527344, -0.20214844,  0.01953125, -0.45898438,  0.21191406,\n",
       "        -0.15429688,  0.3046875 , -0.00897217,  0.1640625 ,  0.04077148,\n",
       "         0.15136719, -0.01025391,  0.12695312, -0.22851562, -0.11767578,\n",
       "        -0.01239014,  0.33789062, -0.09228516,  0.0390625 ,  0.05761719,\n",
       "        -0.01928711,  0.13769531, -0.00209045, -0.24707031,  0.04516602,\n",
       "         0.31054688, -0.01904297, -0.05102539, -0.04003906, -0.20898438,\n",
       "         0.15625   ,  0.140625  , -0.07080078,  0.24414062, -0.20703125,\n",
       "         0.02563477,  0.09228516,  0.10449219,  0.04614258,  0.00704956,\n",
       "        -0.171875  ,  0.10449219, -0.30664062,  0.10009766,  0.22753906,\n",
       "         0.09619141,  0.01831055,  0.140625  , -0.09570312,  0.13378906,\n",
       "        -0.16210938,  0.16308594,  0.11230469,  0.04345703,  0.16601562,\n",
       "        -0.00108337,  0.1875    , -0.015625  , -0.05834961, -0.15429688,\n",
       "         0.09814453,  0.09912109,  0.01586914,  0.08691406, -0.03039551,\n",
       "        -0.03540039, -0.1484375 , -0.24804688, -0.25976562,  0.08105469,\n",
       "        -0.11425781, -0.18554688,  0.13769531, -0.15917969,  0.04174805,\n",
       "        -0.08251953,  0.05273438, -0.0546875 ,  0.07275391, -0.12402344,\n",
       "         0.12207031, -0.24707031, -0.10742188, -0.18457031,  0.05615234,\n",
       "        -0.10253906,  0.02563477,  0.03015137, -0.33789062, -0.0625    ,\n",
       "        -0.1640625 , -0.11474609, -0.19921875,  0.04199219, -0.07275391,\n",
       "        -0.27734375,  0.09619141, -0.07861328, -0.09716797, -0.03295898,\n",
       "         0.11523438, -0.01977539, -0.01208496, -0.16894531,  0.0390625 ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Extracting dense features...')\n",
    "dense_feature_representations[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abb025ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a37da960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4d05247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76db6d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1488      12    125     11      7       0     14    116     64\n",
      "B-MISC        34     662     33     12      0      11     14     45    111\n",
      "B-ORG         85      17    888     15     13       3     68    137    115\n",
      "B-PER         17       3     26   1166      0       0      7    518    105\n",
      "I-LOC         20       2      4      0    169       4     17     25     16\n",
      "I-MISC         2      31     18      1      9     169      7     31     78\n",
      "I-ORG         45       5     64     12     41       5    360     83    136\n",
      "I-PER         13       3     16    120      0       0      7   1108     40\n",
      "O              6      11     68     11      1       9     26    158  42468\n",
      "gold           0       0      0      0      0       0      0      0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.6599033972113301 R: 0.5870458883302929 F1: 0.617882004486319\n"
     ]
    }
   ],
   "source": [
    "predicted, gold = label_data_using_word_embeddings(testfile, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8629c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "\n",
    "\n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "    return combined_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c5b8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) > 6:\n",
    "            token_vector = extract_word_embedding(row[0], word_embedding_model)\n",
    "            pt_vector = extract_word_embedding(row[1], word_embedding_model)\n",
    "            dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "            #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "            #we thus only use other features with limited values\n",
    "            other_features = extract_feature_values(row, ['capitals','pos','tag'])\n",
    "            traditional_features.append(other_features)\n",
    "            #adding gold label to labels\n",
    "            labels.append(row[3])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a6fd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78cbdfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features...\n",
      "Training classifier....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1516      22    156     28     18       2     18     18     59\n",
      "B-MISC        33     669     42     15      3      15     15     19    111\n",
      "B-ORG        109      38    919     57     13       8     80     29     88\n",
      "B-PER         30       8     36   1518      9       0     10    154     77\n",
      "I-LOC         13       0      6      3    158       5     40     14     18\n",
      "I-MISC         4      34     16      2     10     167     11     13     89\n",
      "I-ORG         39      14     73     15     39      13    366     45    147\n",
      "I-PER          7       3     16     84      3       5     16   1140     33\n",
      "O              6      36     73     11      7      16     27    127  42455\n",
      "gold           0       0      0      0      0       0      0      0      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melis Nur\\myPython\\Natural-Language-Processing\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.691322419392516 R: 0.6510204039255932 F1: 0.6669061336101958\n"
     ]
    }
   ],
   "source": [
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(trainfile, word_embedding_model)\n",
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels)\n",
    "print('Running the evaluation...')\n",
    "predictions, goldlabels = label_data_with_combined_features(testfile, lr_classifier, vectorizer, word_embedding_model)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeae2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
